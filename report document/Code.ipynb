{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SvPQAWreT63x"
   },
   "source": [
    "# Udacity Capstone Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "le3WdOCST7Xm"
   },
   "source": [
    "## EEG signal Based Eye State Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZtrY0ulL5Zl2"
   },
   "source": [
    "### Loading the files in Google colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "i2QS1zSNfF8k"
   },
   "outputs": [],
   "source": [
    "# You can skip this step if you are training on local System\n",
    "\n",
    "!pip install -U -q PyDrive\n",
    "from pydrive.auth import GoogleAuth\n",
    "from pydrive.drive import GoogleDrive\n",
    "from google.colab import auth\n",
    "from oauth2client.client import GoogleCredentials\n",
    "\n",
    "auth.authenticate_user()\n",
    "gauth = GoogleAuth()\n",
    "gauth.credentials = GoogleCredentials.get_application_default()\n",
    "drive = GoogleDrive(gauth)\n",
    "\n",
    "fid = drive.ListFile({'q':\"title='EEG Eye State.txt'\"}).GetList()[0]['id']\n",
    "f = drive.CreateFile({'id': fid})\n",
    "f.GetContentFile('EEG Eye State.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "60hZQgdqflKU"
   },
   "outputs": [],
   "source": [
    "# Loading the dataset\n",
    "\n",
    "fname = \"EEG Eye State.txt\"\n",
    "with open(fname) as f:\n",
    "    content = f.readlines()\n",
    "\n",
    "content = [x.strip() for x in content] \n",
    "content = [x.split(\",\") for x in content]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "aYVDStrLgnid"
   },
   "outputs": [],
   "source": [
    "# Converting list to numpy array\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "content = np.array(content, dtype = 'float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "W1m_uyCfse2y"
   },
   "outputs": [],
   "source": [
    "# Shuffling the dataset\n",
    "\n",
    "import random\n",
    "random.shuffle(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "LwqzamB1XFmk"
   },
   "outputs": [],
   "source": [
    "# Storing results of algorithms\n",
    "score_p = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "S41pxHVJ5epZ"
   },
   "source": [
    "### Creating X and Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "DjnxZvhpgqKy"
   },
   "outputs": [],
   "source": [
    "x = content[:, :-1]\n",
    "y = np.array(content[:, -1], dtype = 'int32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 73
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 866,
     "status": "ok",
     "timestamp": 1528838208612,
     "user": {
      "displayName": "SHUBHAM VERMA",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "108363614808362191748"
     },
     "user_tz": -330
    },
    "id": "BVr2ve7Ckp1U",
    "outputId": "0c09278d-c6db-4414-803b-8afb7c6a736e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4329.23, 4009.23, 4289.23, 4148.21, 4350.26, 4586.15, 4096.92,\n",
       "       4641.03, 4222.05, 4238.46, 4211.28, 4280.51, 4635.9 , 4393.85],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 128,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 775,
     "status": "ok",
     "timestamp": 1528838211087,
     "user": {
      "displayName": "SHUBHAM VERMA",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "108363614808362191748"
     },
     "user_tz": -330
    },
    "id": "JTySF2jYUyrO",
    "outputId": "2bef2805-02bb-4180-f6c9-397f60175b41"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 129,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "X1tcVoh6kq9p"
   },
   "source": [
    "# Statistical Features approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ie0mMLieqB2t"
   },
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "Tw8m8Wsklewm"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "CkOJrTBwkqQG"
   },
   "outputs": [],
   "source": [
    "X_columns = ['mean', 'standard deviation', 'kurt', 'skewness']\n",
    "Y_columns = ['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "zlMuSMqSkqUJ"
   },
   "outputs": [],
   "source": [
    "X = pd.DataFrame(columns = X_columns)\n",
    "Y = pd.DataFrame(columns = Y_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "-SkGMSZHkqOF"
   },
   "outputs": [],
   "source": [
    "for i in range(len(x)):\n",
    "  X.loc[i] = np.array([np.mean(x[i]), np.std(x[i]), scipy.stats.kurtosis(x[i]), scipy.stats.skew(x[i])])\n",
    "  Y.loc[i] = y[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 677
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 794,
     "status": "ok",
     "timestamp": 1528838298864,
     "user": {
      "displayName": "SHUBHAM VERMA",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "108363614808362191748"
     },
     "user_tz": -330
    },
    "id": "kkpqklZPowkK",
    "outputId": "34ee5ed1-0bdf-4d6f-91f1-6d4f7958235f"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>standard deviation</th>\n",
       "      <th>kurt</th>\n",
       "      <th>skewness</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4316.593750</td>\n",
       "      <td>186.395279</td>\n",
       "      <td>-0.686817</td>\n",
       "      <td>0.439234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4312.748535</td>\n",
       "      <td>186.797379</td>\n",
       "      <td>-0.673855</td>\n",
       "      <td>0.459162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4312.748535</td>\n",
       "      <td>186.797379</td>\n",
       "      <td>-0.673855</td>\n",
       "      <td>0.459162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4312.748535</td>\n",
       "      <td>186.797379</td>\n",
       "      <td>-0.673855</td>\n",
       "      <td>0.459162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4316.189941</td>\n",
       "      <td>184.204300</td>\n",
       "      <td>-0.716226</td>\n",
       "      <td>0.414606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4312.160645</td>\n",
       "      <td>184.608978</td>\n",
       "      <td>-0.693921</td>\n",
       "      <td>0.443294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>4312.748535</td>\n",
       "      <td>186.797379</td>\n",
       "      <td>-0.673855</td>\n",
       "      <td>0.459162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>4316.227051</td>\n",
       "      <td>183.612762</td>\n",
       "      <td>-0.696899</td>\n",
       "      <td>0.422589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>4312.160645</td>\n",
       "      <td>184.608978</td>\n",
       "      <td>-0.693921</td>\n",
       "      <td>0.443294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>4307.143555</td>\n",
       "      <td>183.993057</td>\n",
       "      <td>-0.686661</td>\n",
       "      <td>0.435931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>4307.143555</td>\n",
       "      <td>183.993057</td>\n",
       "      <td>-0.686661</td>\n",
       "      <td>0.435931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>4300.036621</td>\n",
       "      <td>184.229202</td>\n",
       "      <td>-0.721467</td>\n",
       "      <td>0.474390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>4307.143555</td>\n",
       "      <td>183.993057</td>\n",
       "      <td>-0.686661</td>\n",
       "      <td>0.435931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>4316.227051</td>\n",
       "      <td>183.612762</td>\n",
       "      <td>-0.696899</td>\n",
       "      <td>0.422589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>4300.036621</td>\n",
       "      <td>184.229202</td>\n",
       "      <td>-0.721467</td>\n",
       "      <td>0.474390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>4302.344238</td>\n",
       "      <td>185.532516</td>\n",
       "      <td>-0.719003</td>\n",
       "      <td>0.481021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>4316.227051</td>\n",
       "      <td>183.612762</td>\n",
       "      <td>-0.696899</td>\n",
       "      <td>0.422589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>4297.360840</td>\n",
       "      <td>185.055405</td>\n",
       "      <td>-0.704686</td>\n",
       "      <td>0.497390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>4303.917969</td>\n",
       "      <td>185.143890</td>\n",
       "      <td>-0.711733</td>\n",
       "      <td>0.475173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>4302.344238</td>\n",
       "      <td>185.532516</td>\n",
       "      <td>-0.719003</td>\n",
       "      <td>0.481021</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           mean  standard deviation      kurt  skewness\n",
       "0   4316.593750          186.395279 -0.686817  0.439234\n",
       "1   4312.748535          186.797379 -0.673855  0.459162\n",
       "2   4312.748535          186.797379 -0.673855  0.459162\n",
       "3   4312.748535          186.797379 -0.673855  0.459162\n",
       "4   4316.189941          184.204300 -0.716226  0.414606\n",
       "5   4312.160645          184.608978 -0.693921  0.443294\n",
       "6   4312.748535          186.797379 -0.673855  0.459162\n",
       "7   4316.227051          183.612762 -0.696899  0.422589\n",
       "8   4312.160645          184.608978 -0.693921  0.443294\n",
       "9   4307.143555          183.993057 -0.686661  0.435931\n",
       "10  4307.143555          183.993057 -0.686661  0.435931\n",
       "11  4300.036621          184.229202 -0.721467  0.474390\n",
       "12  4307.143555          183.993057 -0.686661  0.435931\n",
       "13  4316.227051          183.612762 -0.696899  0.422589\n",
       "14  4300.036621          184.229202 -0.721467  0.474390\n",
       "15  4302.344238          185.532516 -0.719003  0.481021\n",
       "16  4316.227051          183.612762 -0.696899  0.422589\n",
       "17  4297.360840          185.055405 -0.704686  0.497390\n",
       "18  4303.917969          185.143890 -0.711733  0.475173\n",
       "19  4302.344238          185.532516 -0.719003  0.481021"
      ]
     },
     "execution_count": 134,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head(n=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 800,
     "status": "ok",
     "timestamp": 1528838301215,
     "user": {
      "displayName": "SHUBHAM VERMA",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "108363614808362191748"
     },
     "user_tz": -330
    },
    "id": "Exz5fojXo1ZD",
    "outputId": "c19aff3b-9f82-4c35-d727-b348bc0e78a2"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label\n",
       "0     0\n",
       "1     0\n",
       "2     0\n",
       "3     0\n",
       "4     0"
      ]
     },
     "execution_count": 135,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "ZNr_a9BSpe2M"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the 'features' and 'income' data into training and testing sets\n",
    "X_train1, X_test1, y_train1, y_test1 = train_test_split(X, \n",
    "                                                    y, \n",
    "                                                    test_size = 0.2, \n",
    "                                                    random_state = 0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tOVKmTlm975G"
   },
   "source": [
    "## Training on SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 91
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 8633,
     "status": "ok",
     "timestamp": 1528838313637,
     "user": {
      "displayName": "SHUBHAM VERMA",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "108363614808362191748"
     },
     "user_tz": -330
    },
    "id": "paKVcOIqpe6R",
    "outputId": "ca6c31e3-be10-4883-d6a3-34a28b6187d4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 137,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "clf = SVC()\n",
    "clf.fit(X_train1, y_train1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "cFTjEv8wpezm"
   },
   "outputs": [],
   "source": [
    "predicted = clf.predict(X_test1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "ctpAFaeo8y5u"
   },
   "outputs": [],
   "source": [
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fA32BmL1VHdS"
   },
   "source": [
    "### Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 91
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 685,
     "status": "ok",
     "timestamp": 1528839731562,
     "user": {
      "displayName": "SHUBHAM VERMA",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "108363614808362191748"
     },
     "user_tz": -330
    },
    "id": "gaISrM048y2n",
    "outputId": "2aa18b1b-4dc3-4892-9026-4eda9fae30ed"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.6989319092122831\n",
      "Precision = 0.7059190031152648\n",
      "Recall = 0.7248880358285349\n",
      "F1 Score = 0.7152777777777777\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy = {}\\nPrecision = {}\\nRecall = {}\\nF1 Score = {}\".format(metrics.accuracy_score(y_test1, predicted), metrics.precision_score(y_test1, predicted),metrics.recall_score(y_test1, predicted),metrics.f1_score(y_test1, predicted)))\n",
    "\n",
    "score_p.append([metrics.accuracy_score(y_test1, predicted), metrics.precision_score(y_test1, predicted),metrics.recall_score(y_test1, predicted),metrics.f1_score(y_test1, predicted)]) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "q4u3LnbbsNR8"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aDvHS8kU-JUh"
   },
   "source": [
    "## Training on Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "pc4_RzElmvdZ"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 111
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 784,
     "status": "ok",
     "timestamp": 1528838337090,
     "user": {
      "displayName": "SHUBHAM VERMA",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "108363614808362191748"
     },
     "user_tz": -330
    },
    "id": "ckjIJXyF-ccn",
    "outputId": "78bef499-c8ab-4d1d-c214-27c21dab3041"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 142,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_D = LogisticRegression()\n",
    "clf_D.fit(X_train1, y_train1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "PGT09n8G_U0e"
   },
   "outputs": [],
   "source": [
    "predict = clf_D.predict(X_test1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1cch7p1zVPCO"
   },
   "source": [
    "### Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 91
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1016,
     "status": "ok",
     "timestamp": 1528839743803,
     "user": {
      "displayName": "SHUBHAM VERMA",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "108363614808362191748"
     },
     "user_tz": -330
    },
    "id": "Phhv-p2p_VAk",
    "outputId": "db3f9548-72c9-4c7e-8f49-77983a4ec9d5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.5240320427236315\n",
      "Precision = 0.5404607206142942\n",
      "Recall = 0.5854126679462572\n",
      "F1 Score = 0.5620393120393121\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy = {}\\nPrecision = {}\\nRecall = {}\\nF1 Score = {}\".format(metrics.accuracy_score(y_test1, predict), metrics.precision_score(y_test1, predict),metrics.recall_score(y_test1, predict),metrics.f1_score(y_test1, predict)))\n",
    "score_p.append([metrics.accuracy_score(y_test1, predict), metrics.precision_score(y_test1, predict),metrics.recall_score(y_test1, predict),metrics.f1_score(y_test1, predict)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GnFFg0vtVeCH"
   },
   "source": [
    "## Second Approach Directly use 14 values of EEG data and use it for prediction "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sgqdtQHV6Ce7"
   },
   "source": [
    "### Normalization of data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6odWgZ7wVWGF"
   },
   "source": [
    "Normalization of data is required before using it on Nueral Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "qwpiaNsqg7EU"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "F1oNrbyBiO0z"
   },
   "outputs": [],
   "source": [
    "scaler.fit(x)\n",
    "x_new = scaler.transform(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "kUrGdTA2Nvjk"
   },
   "outputs": [],
   "source": [
    "data_mean = x.mean()\n",
    "data_std = x.std()\n",
    "x = (x - data_mean)/data_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "cIhNcStuldzi"
   },
   "outputs": [],
   "source": [
    "(x_train, x_test) = x[:11000], x[11000:]\n",
    "(y_train, y_test) = y[:11000], y[11000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hCJlgBEy58II"
   },
   "source": [
    "## Training on Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MN_--9h56Mi9"
   },
   "source": [
    "### Creating Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "G8tW2xROi6lz"
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 221
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 836,
     "status": "ok",
     "timestamp": 1528838368502,
     "user": {
      "displayName": "SHUBHAM VERMA",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "108363614808362191748"
     },
     "user_tz": -330
    },
    "id": "4AxUgQBli8xN",
    "outputId": "307361bc-640f-4bef-de28-13600b6523e2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_39 (Dense)             (None, 512)               7680      \n",
      "_________________________________________________________________\n",
      "dense_40 (Dense)             (None, 1)                 513       \n",
      "=================================================================\n",
      "Total params: 8,193\n",
      "Trainable params: 8,193\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(512, activation = 'relu', input_shape = (x.shape[1], )))\n",
    "model.add(Dense(1, activation = 'sigmoid'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 221
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 733,
     "status": "ok",
     "timestamp": 1528838372215,
     "user": {
      "displayName": "SHUBHAM VERMA",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "108363614808362191748"
     },
     "user_tz": -330
    },
    "id": "Lpwu-Tg3jZr2",
    "outputId": "1895fc93-dd34-495a-c551-10a06227d411"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_39 (Dense)             (None, 512)               7680      \n",
      "_________________________________________________________________\n",
      "dense_40 (Dense)             (None, 1)                 513       \n",
      "=================================================================\n",
      "Total params: 8,193\n",
      "Trainable params: 8,193\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss = 'binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "N9hvDrSn4y4N"
   },
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 7482
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 27056,
     "status": "ok",
     "timestamp": 1528838410588,
     "user": {
      "displayName": "SHUBHAM VERMA",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "108363614808362191748"
     },
     "user_tz": -330
    },
    "id": "vKh20CJrmYV7",
    "outputId": "55aa30fa-fdf5-45a6-b6ed-3461156bc844"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9900 samples, validate on 1100 samples\n",
      "Epoch 1/100\n",
      " - 1s - loss: 0.6908 - acc: 0.5247 - val_loss: 0.6909 - val_acc: 0.5255\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.69085, saving model to MLP.weights.best.hdf5\n",
      "Epoch 2/100\n",
      " - 0s - loss: 0.6891 - acc: 0.5276 - val_loss: 0.6900 - val_acc: 0.5400\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.69085 to 0.68995, saving model to MLP.weights.best.hdf5\n",
      "Epoch 3/100\n",
      " - 0s - loss: 0.6867 - acc: 0.5356 - val_loss: 0.6896 - val_acc: 0.5500\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.68995 to 0.68959, saving model to MLP.weights.best.hdf5\n",
      "Epoch 4/100\n",
      " - 0s - loss: 0.6850 - acc: 0.5458 - val_loss: 0.6877 - val_acc: 0.5427\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.68959 to 0.68769, saving model to MLP.weights.best.hdf5\n",
      "Epoch 5/100\n",
      " - 0s - loss: 0.6816 - acc: 0.5536 - val_loss: 0.6866 - val_acc: 0.5482\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.68769 to 0.68658, saving model to MLP.weights.best.hdf5\n",
      "Epoch 6/100\n",
      " - 0s - loss: 0.6783 - acc: 0.5560 - val_loss: 0.6852 - val_acc: 0.5436\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.68658 to 0.68519, saving model to MLP.weights.best.hdf5\n",
      "Epoch 7/100\n",
      " - 0s - loss: 0.6749 - acc: 0.5598 - val_loss: 0.6838 - val_acc: 0.5609\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.68519 to 0.68375, saving model to MLP.weights.best.hdf5\n",
      "Epoch 8/100\n",
      " - 0s - loss: 0.6723 - acc: 0.5654 - val_loss: 0.6845 - val_acc: 0.5445\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.68375\n",
      "Epoch 9/100\n",
      " - 0s - loss: 0.6686 - acc: 0.5651 - val_loss: 0.6844 - val_acc: 0.5318\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.68375\n",
      "Epoch 10/100\n",
      " - 0s - loss: 0.6652 - acc: 0.5718 - val_loss: 0.6822 - val_acc: 0.5409\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.68375 to 0.68220, saving model to MLP.weights.best.hdf5\n",
      "Epoch 11/100\n",
      " - 0s - loss: 0.6623 - acc: 0.5694 - val_loss: 0.6829 - val_acc: 0.5345\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.68220\n",
      "Epoch 12/100\n",
      " - 0s - loss: 0.6593 - acc: 0.5688 - val_loss: 0.6826 - val_acc: 0.5291\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.68220\n",
      "Epoch 13/100\n",
      " - 0s - loss: 0.6562 - acc: 0.5817 - val_loss: 0.6783 - val_acc: 0.5409\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.68220 to 0.67825, saving model to MLP.weights.best.hdf5\n",
      "Epoch 14/100\n",
      " - 0s - loss: 0.6536 - acc: 0.5808 - val_loss: 0.6791 - val_acc: 0.5545\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.67825\n",
      "Epoch 15/100\n",
      " - 0s - loss: 0.6506 - acc: 0.5818 - val_loss: 0.6776 - val_acc: 0.5455\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.67825 to 0.67760, saving model to MLP.weights.best.hdf5\n",
      "Epoch 16/100\n",
      " - 0s - loss: 0.6485 - acc: 0.5881 - val_loss: 0.6832 - val_acc: 0.5282\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.67760\n",
      "Epoch 17/100\n",
      " - 0s - loss: 0.6453 - acc: 0.5925 - val_loss: 0.6811 - val_acc: 0.5436\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.67760\n",
      "Epoch 18/100\n",
      " - 0s - loss: 0.6426 - acc: 0.5958 - val_loss: 0.6783 - val_acc: 0.5636\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.67760\n",
      "Epoch 19/100\n",
      " - 0s - loss: 0.6405 - acc: 0.6051 - val_loss: 0.6783 - val_acc: 0.5691\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.67760\n",
      "Epoch 20/100\n",
      " - 0s - loss: 0.6379 - acc: 0.6047 - val_loss: 0.6765 - val_acc: 0.5664\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.67760 to 0.67654, saving model to MLP.weights.best.hdf5\n",
      "Epoch 21/100\n",
      " - 0s - loss: 0.6354 - acc: 0.6080 - val_loss: 0.6774 - val_acc: 0.5655\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.67654\n",
      "Epoch 22/100\n",
      " - 0s - loss: 0.6327 - acc: 0.6166 - val_loss: 0.6733 - val_acc: 0.5718\n",
      "\n",
      "Epoch 00022: val_loss improved from 0.67654 to 0.67333, saving model to MLP.weights.best.hdf5\n",
      "Epoch 23/100\n",
      " - 0s - loss: 0.6314 - acc: 0.6058 - val_loss: 0.6754 - val_acc: 0.5664\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.67333\n",
      "Epoch 24/100\n",
      " - 0s - loss: 0.6298 - acc: 0.6194 - val_loss: 0.6712 - val_acc: 0.5627\n",
      "\n",
      "Epoch 00024: val_loss improved from 0.67333 to 0.67123, saving model to MLP.weights.best.hdf5\n",
      "Epoch 25/100\n",
      " - 0s - loss: 0.6272 - acc: 0.6124 - val_loss: 0.6723 - val_acc: 0.5736\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.67123\n",
      "Epoch 26/100\n",
      " - 0s - loss: 0.6252 - acc: 0.6231 - val_loss: 0.6737 - val_acc: 0.5664\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.67123\n",
      "Epoch 27/100\n",
      " - 0s - loss: 0.6238 - acc: 0.6216 - val_loss: 0.6716 - val_acc: 0.5709\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.67123\n",
      "Epoch 28/100\n",
      " - 0s - loss: 0.6216 - acc: 0.6270 - val_loss: 0.6750 - val_acc: 0.5664\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.67123\n",
      "Epoch 29/100\n",
      " - 0s - loss: 0.6215 - acc: 0.6288 - val_loss: 0.6714 - val_acc: 0.5627\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.67123\n",
      "Epoch 30/100\n",
      " - 0s - loss: 0.6188 - acc: 0.6252 - val_loss: 0.6714 - val_acc: 0.5718\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.67123\n",
      "Epoch 31/100\n",
      " - 0s - loss: 0.6171 - acc: 0.6297 - val_loss: 0.6799 - val_acc: 0.5473\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.67123\n",
      "Epoch 32/100\n",
      " - 0s - loss: 0.6159 - acc: 0.6333 - val_loss: 0.6706 - val_acc: 0.5718\n",
      "\n",
      "Epoch 00032: val_loss improved from 0.67123 to 0.67061, saving model to MLP.weights.best.hdf5\n",
      "Epoch 33/100\n",
      " - 0s - loss: 0.6143 - acc: 0.6327 - val_loss: 0.6690 - val_acc: 0.5664\n",
      "\n",
      "Epoch 00033: val_loss improved from 0.67061 to 0.66902, saving model to MLP.weights.best.hdf5\n",
      "Epoch 34/100\n",
      " - 0s - loss: 0.6132 - acc: 0.6288 - val_loss: 0.6687 - val_acc: 0.5773\n",
      "\n",
      "Epoch 00034: val_loss improved from 0.66902 to 0.66867, saving model to MLP.weights.best.hdf5\n",
      "Epoch 35/100\n",
      " - 0s - loss: 0.6112 - acc: 0.6414 - val_loss: 0.6705 - val_acc: 0.5664\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.66867\n",
      "Epoch 36/100\n",
      " - 0s - loss: 0.6095 - acc: 0.6376 - val_loss: 0.6718 - val_acc: 0.5700\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.66867\n",
      "Epoch 37/100\n",
      " - 0s - loss: 0.6093 - acc: 0.6358 - val_loss: 0.6809 - val_acc: 0.5500\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.66867\n",
      "Epoch 38/100\n",
      " - 0s - loss: 0.6085 - acc: 0.6386 - val_loss: 0.6785 - val_acc: 0.5636\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.66867\n",
      "Epoch 39/100\n",
      " - 0s - loss: 0.6059 - acc: 0.6407 - val_loss: 0.6689 - val_acc: 0.5664\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.66867\n",
      "Epoch 40/100\n",
      " - 0s - loss: 0.6055 - acc: 0.6439 - val_loss: 0.6685 - val_acc: 0.5691\n",
      "\n",
      "Epoch 00040: val_loss improved from 0.66867 to 0.66854, saving model to MLP.weights.best.hdf5\n",
      "Epoch 41/100\n",
      " - 0s - loss: 0.6049 - acc: 0.6415 - val_loss: 0.6690 - val_acc: 0.5664\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.66854\n",
      "Epoch 42/100\n",
      " - 0s - loss: 0.6028 - acc: 0.6433 - val_loss: 0.6774 - val_acc: 0.5645\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.66854\n",
      "Epoch 43/100\n",
      " - 0s - loss: 0.6017 - acc: 0.6477 - val_loss: 0.6671 - val_acc: 0.5800\n",
      "\n",
      "Epoch 00043: val_loss improved from 0.66854 to 0.66706, saving model to MLP.weights.best.hdf5\n",
      "Epoch 44/100\n",
      " - 0s - loss: 0.6006 - acc: 0.6493 - val_loss: 0.6691 - val_acc: 0.5718\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.66706\n",
      "Epoch 45/100\n",
      " - 0s - loss: 0.6007 - acc: 0.6499 - val_loss: 0.6702 - val_acc: 0.5718\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.66706\n",
      "Epoch 46/100\n",
      " - 0s - loss: 0.5991 - acc: 0.6544 - val_loss: 0.6714 - val_acc: 0.5673\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.66706\n",
      "Epoch 47/100\n",
      " - 0s - loss: 0.5970 - acc: 0.6563 - val_loss: 0.6673 - val_acc: 0.5764\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.66706\n",
      "Epoch 48/100\n",
      " - 0s - loss: 0.5973 - acc: 0.6555 - val_loss: 0.6664 - val_acc: 0.5691\n",
      "\n",
      "Epoch 00048: val_loss improved from 0.66706 to 0.66636, saving model to MLP.weights.best.hdf5\n",
      "Epoch 49/100\n",
      " - 0s - loss: 0.5950 - acc: 0.6533 - val_loss: 0.6720 - val_acc: 0.5573\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.66636\n",
      "Epoch 50/100\n",
      " - 0s - loss: 0.5939 - acc: 0.6582 - val_loss: 0.6704 - val_acc: 0.5636\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.66636\n",
      "Epoch 51/100\n",
      " - 0s - loss: 0.5921 - acc: 0.6610 - val_loss: 0.6687 - val_acc: 0.5682\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.66636\n",
      "Epoch 52/100\n",
      " - 0s - loss: 0.5914 - acc: 0.6619 - val_loss: 0.6646 - val_acc: 0.5855\n",
      "\n",
      "Epoch 00052: val_loss improved from 0.66636 to 0.66464, saving model to MLP.weights.best.hdf5\n",
      "Epoch 53/100\n",
      " - 0s - loss: 0.5903 - acc: 0.6630 - val_loss: 0.6652 - val_acc: 0.5727\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.66464\n",
      "Epoch 54/100\n",
      " - 0s - loss: 0.5902 - acc: 0.6655 - val_loss: 0.6650 - val_acc: 0.5845\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.66464\n",
      "Epoch 55/100\n",
      " - 0s - loss: 0.5904 - acc: 0.6682 - val_loss: 0.6801 - val_acc: 0.5536\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.66464\n",
      "Epoch 56/100\n",
      " - 0s - loss: 0.5877 - acc: 0.6681 - val_loss: 0.6673 - val_acc: 0.5718\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.66464\n",
      "Epoch 57/100\n",
      " - 0s - loss: 0.5864 - acc: 0.6684 - val_loss: 0.6649 - val_acc: 0.5836\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.66464\n",
      "Epoch 58/100\n",
      " - 0s - loss: 0.5860 - acc: 0.6699 - val_loss: 0.6680 - val_acc: 0.5718\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.66464\n",
      "Epoch 59/100\n",
      " - 0s - loss: 0.5848 - acc: 0.6775 - val_loss: 0.6711 - val_acc: 0.5645\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.66464\n",
      "Epoch 60/100\n",
      " - 0s - loss: 0.5836 - acc: 0.6715 - val_loss: 0.6636 - val_acc: 0.5800\n",
      "\n",
      "Epoch 00060: val_loss improved from 0.66464 to 0.66359, saving model to MLP.weights.best.hdf5\n",
      "Epoch 61/100\n",
      " - 0s - loss: 0.5826 - acc: 0.6766 - val_loss: 0.6644 - val_acc: 0.5827\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.66359\n",
      "Epoch 62/100\n",
      " - 0s - loss: 0.5807 - acc: 0.6818 - val_loss: 0.6645 - val_acc: 0.5809\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.66359\n",
      "Epoch 63/100\n",
      " - 0s - loss: 0.5809 - acc: 0.6794 - val_loss: 0.6715 - val_acc: 0.5645\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.66359\n",
      "Epoch 64/100\n",
      " - 0s - loss: 0.5786 - acc: 0.6879 - val_loss: 0.6641 - val_acc: 0.5809\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 0.66359\n",
      "Epoch 65/100\n",
      " - 0s - loss: 0.5790 - acc: 0.6863 - val_loss: 0.6724 - val_acc: 0.5627\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 0.66359\n",
      "Epoch 66/100\n",
      " - 0s - loss: 0.5784 - acc: 0.6854 - val_loss: 0.6622 - val_acc: 0.5918\n",
      "\n",
      "Epoch 00066: val_loss improved from 0.66359 to 0.66219, saving model to MLP.weights.best.hdf5\n",
      "Epoch 67/100\n",
      " - 0s - loss: 0.5761 - acc: 0.6893 - val_loss: 0.6702 - val_acc: 0.5700\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.66219\n",
      "Epoch 68/100\n",
      " - 0s - loss: 0.5752 - acc: 0.6917 - val_loss: 0.6631 - val_acc: 0.5818\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.66219\n",
      "Epoch 69/100\n",
      " - 0s - loss: 0.5749 - acc: 0.6908 - val_loss: 0.6700 - val_acc: 0.5718\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.66219\n",
      "Epoch 70/100\n",
      " - 0s - loss: 0.5750 - acc: 0.6964 - val_loss: 0.6624 - val_acc: 0.5864\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.66219\n",
      "Epoch 71/100\n",
      " - 0s - loss: 0.5738 - acc: 0.6942 - val_loss: 0.6634 - val_acc: 0.5864\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.66219\n",
      "Epoch 72/100\n",
      " - 0s - loss: 0.5713 - acc: 0.6963 - val_loss: 0.6657 - val_acc: 0.5755\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.66219\n",
      "Epoch 73/100\n",
      " - 0s - loss: 0.5705 - acc: 0.7034 - val_loss: 0.6625 - val_acc: 0.5891\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 0.66219\n",
      "Epoch 74/100\n",
      " - 0s - loss: 0.5702 - acc: 0.7017 - val_loss: 0.6686 - val_acc: 0.5700\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 0.66219\n",
      "Epoch 75/100\n",
      " - 0s - loss: 0.5727 - acc: 0.6845 - val_loss: 0.6767 - val_acc: 0.5591\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 0.66219\n",
      "Epoch 76/100\n",
      " - 0s - loss: 0.5723 - acc: 0.6943 - val_loss: 0.6616 - val_acc: 0.5927\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00076: val_loss improved from 0.66219 to 0.66156, saving model to MLP.weights.best.hdf5\n",
      "Epoch 77/100\n",
      " - 0s - loss: 0.5680 - acc: 0.7068 - val_loss: 0.6634 - val_acc: 0.5864\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 0.66156\n",
      "Epoch 78/100\n",
      " - 0s - loss: 0.5661 - acc: 0.7090 - val_loss: 0.6634 - val_acc: 0.5873\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 0.66156\n",
      "Epoch 79/100\n",
      " - 0s - loss: 0.5656 - acc: 0.7166 - val_loss: 0.6625 - val_acc: 0.5836\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 0.66156\n",
      "Epoch 80/100\n",
      " - 0s - loss: 0.5633 - acc: 0.7212 - val_loss: 0.6611 - val_acc: 0.5927\n",
      "\n",
      "Epoch 00080: val_loss improved from 0.66156 to 0.66105, saving model to MLP.weights.best.hdf5\n",
      "Epoch 81/100\n",
      " - 0s - loss: 0.5649 - acc: 0.7073 - val_loss: 0.6613 - val_acc: 0.5955\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 0.66105\n",
      "Epoch 82/100\n",
      " - 0s - loss: 0.5636 - acc: 0.7161 - val_loss: 0.6610 - val_acc: 0.5936\n",
      "\n",
      "Epoch 00082: val_loss improved from 0.66105 to 0.66103, saving model to MLP.weights.best.hdf5\n",
      "Epoch 83/100\n",
      " - 0s - loss: 0.5618 - acc: 0.7178 - val_loss: 0.6611 - val_acc: 0.5945\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 0.66103\n",
      "Epoch 84/100\n",
      " - 0s - loss: 0.5604 - acc: 0.7247 - val_loss: 0.6636 - val_acc: 0.5909\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 0.66103\n",
      "Epoch 85/100\n",
      " - 0s - loss: 0.5614 - acc: 0.7190 - val_loss: 0.6621 - val_acc: 0.5909\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 0.66103\n",
      "Epoch 86/100\n",
      " - 0s - loss: 0.5607 - acc: 0.7166 - val_loss: 0.6650 - val_acc: 0.5864\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 0.66103\n",
      "Epoch 87/100\n",
      " - 0s - loss: 0.5595 - acc: 0.7216 - val_loss: 0.6703 - val_acc: 0.5773\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 0.66103\n",
      "Epoch 88/100\n",
      " - 0s - loss: 0.5577 - acc: 0.7236 - val_loss: 0.6610 - val_acc: 0.6018\n",
      "\n",
      "Epoch 00088: val_loss improved from 0.66103 to 0.66103, saving model to MLP.weights.best.hdf5\n",
      "Epoch 89/100\n",
      " - 0s - loss: 0.5560 - acc: 0.7290 - val_loss: 0.6635 - val_acc: 0.5918\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 0.66103\n",
      "Epoch 90/100\n",
      " - 0s - loss: 0.5596 - acc: 0.7151 - val_loss: 0.6631 - val_acc: 0.5918\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 0.66103\n",
      "Epoch 91/100\n",
      " - 0s - loss: 0.5558 - acc: 0.7236 - val_loss: 0.6681 - val_acc: 0.5782\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 0.66103\n",
      "Epoch 92/100\n",
      " - 0s - loss: 0.5566 - acc: 0.7211 - val_loss: 0.6631 - val_acc: 0.5927\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 0.66103\n",
      "Epoch 93/100\n",
      " - 0s - loss: 0.5539 - acc: 0.7332 - val_loss: 0.6633 - val_acc: 0.5909\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 0.66103\n",
      "Epoch 94/100\n",
      " - 0s - loss: 0.5532 - acc: 0.7329 - val_loss: 0.6616 - val_acc: 0.5964\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 0.66103\n",
      "Epoch 95/100\n",
      " - 0s - loss: 0.5529 - acc: 0.7309 - val_loss: 0.6623 - val_acc: 0.5945\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 0.66103\n",
      "Epoch 96/100\n",
      " - 0s - loss: 0.5514 - acc: 0.7351 - val_loss: 0.6617 - val_acc: 0.5900\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 0.66103\n",
      "Epoch 97/100\n",
      " - 0s - loss: 0.5527 - acc: 0.7293 - val_loss: 0.6654 - val_acc: 0.5891\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 0.66103\n",
      "Epoch 98/100\n",
      " - 0s - loss: 0.5509 - acc: 0.7293 - val_loss: 0.6631 - val_acc: 0.5873\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 0.66103\n",
      "Epoch 99/100\n",
      " - 0s - loss: 0.5492 - acc: 0.7393 - val_loss: 0.6619 - val_acc: 0.5945\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 0.66103\n",
      "Epoch 100/100\n",
      " - 0s - loss: 0.5487 - acc: 0.7378 - val_loss: 0.6621 - val_acc: 0.5945\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 0.66103\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath = 'MLP.weights.best.hdf5', verbose = 1, save_best_only = True)\n",
    "hist = model.fit(x_train, y_train, epochs = 100, batch_size=256, validation_split = 0.1, callbacks = [checkpointer], verbose = 2, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1046,
     "status": "ok",
     "timestamp": 1528838421631,
     "user": {
      "displayName": "SHUBHAM VERMA",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "108363614808362191748"
     },
     "user_tz": -330
    },
    "id": "zpdk9x9xngJG",
    "outputId": "40298a3d-369b-4699-ea0e-775cddbc3efd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3980/3980 [==============================] - 0s 69us/step\n",
      "Accuracy:  0.5806532662717542\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(x_test, y_test, verbose=1)\n",
    "print(\"Accuracy: \", score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "zL_el0fmwOPF"
   },
   "outputs": [],
   "source": [
    "predict2 = [1 if a>0.5 else 0 for a in model.predict(x_test)] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OINWUOwYV5A0"
   },
   "source": [
    "### Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 91
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 982,
     "status": "ok",
     "timestamp": 1528838432724,
     "user": {
      "displayName": "SHUBHAM VERMA",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "108363614808362191748"
     },
     "user_tz": -330
    },
    "id": "ScN-ERsEwXjw",
    "outputId": "b8b64d5c-5c76-4772-cd35-aae9a763d916"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.5806532663316583\n",
      "Precision = 0.5720164609053497\n",
      "Recall = 0.6884596334819217\n",
      "F1 Score = 0.624859518993032\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy = {}\\nPrecision = {}\\nRecall = {}\\nF1 Score = {}\".format(metrics.accuracy_score(y_test, predict2), metrics.precision_score(y_test, predict2),metrics.recall_score(y_test, predict2),metrics.f1_score(y_test, predict2)))\n",
    "#score_p.append([metrics.accuracy_score(y_test, predict2), metrics.precision_score(y_test, predict2),metrics.recall_score(y_test, predict2),metrics.f1_score(y_test, predict2)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L9mkRjYIa1iz"
   },
   "source": [
    "# Improved Neural Network (Tuning Parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 407
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 948,
     "status": "ok",
     "timestamp": 1528838441631,
     "user": {
      "displayName": "SHUBHAM VERMA",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "108363614808362191748"
     },
     "user_tz": -330
    },
    "id": "tzNim7esa0g7",
    "outputId": "2541c970-7c8c-49d8-fa2d-744f1138fa2d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_41 (Dense)             (None, 1000)              15000     \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 1000)              0         \n",
      "_________________________________________________________________\n",
      "dense_42 (Dense)             (None, 1000)              1001000   \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, 1000)              0         \n",
      "_________________________________________________________________\n",
      "dense_43 (Dense)             (None, 1000)              1001000   \n",
      "_________________________________________________________________\n",
      "dropout_14 (Dropout)         (None, 1000)              0         \n",
      "_________________________________________________________________\n",
      "dense_44 (Dense)             (None, 1)                 1001      \n",
      "=================================================================\n",
      "Total params: 2,018,001\n",
      "Trainable params: 2,018,001\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model2 = Sequential()\n",
    "model2.add(Dense(1000, activation = 'relu', input_shape = (x.shape[1], )))\n",
    "model2.add(Dropout(0.2))\n",
    "model2.add(Dense(1000, activation = 'relu'))\n",
    "model2.add(Dropout(0.2))\n",
    "model2.add(Dense(1000, activation = 'relu'))\n",
    "model2.add(Dropout(0.2))\n",
    "model2.add(Dense(1, activation = 'sigmoid'))\n",
    "model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 407
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 974,
     "status": "ok",
     "timestamp": 1528838447248,
     "user": {
      "displayName": "SHUBHAM VERMA",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "108363614808362191748"
     },
     "user_tz": -330
    },
    "id": "3DCvBvfKbzpn",
    "outputId": "3ebb0199-df7f-442b-bb40-84760eec4f53"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_41 (Dense)             (None, 1000)              15000     \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 1000)              0         \n",
      "_________________________________________________________________\n",
      "dense_42 (Dense)             (None, 1000)              1001000   \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, 1000)              0         \n",
      "_________________________________________________________________\n",
      "dense_43 (Dense)             (None, 1000)              1001000   \n",
      "_________________________________________________________________\n",
      "dropout_14 (Dropout)         (None, 1000)              0         \n",
      "_________________________________________________________________\n",
      "dense_44 (Dense)             (None, 1)                 1001      \n",
      "=================================================================\n",
      "Total params: 2,018,001\n",
      "Trainable params: 2,018,001\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model2.compile(loss = 'binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 14909
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 53194,
     "status": "ok",
     "timestamp": 1528840496221,
     "user": {
      "displayName": "SHUBHAM VERMA",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "108363614808362191748"
     },
     "user_tz": -330
    },
    "id": "qepgTEuYb206",
    "outputId": "6c696856-6b94-45d3-80c0-f7a13b0d393e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9900 samples, validate on 1100 samples\n",
      "Epoch 1/200\n",
      " - 0s - loss: 0.1822 - acc: 0.9248 - val_loss: 0.6104 - val_acc: 0.8136\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.61037, saving model to MLP_new.weights.best.hdf5\n",
      "Epoch 2/200\n",
      " - 0s - loss: 0.1627 - acc: 0.9368 - val_loss: 0.8544 - val_acc: 0.7718\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.61037\n",
      "Epoch 3/200\n",
      " - 0s - loss: 0.1439 - acc: 0.9425 - val_loss: 0.8382 - val_acc: 0.7927\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.61037\n",
      "Epoch 4/200\n",
      " - 0s - loss: 0.1674 - acc: 0.9303 - val_loss: 0.6309 - val_acc: 0.7709\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.61037\n",
      "Epoch 5/200\n",
      " - 0s - loss: 0.1714 - acc: 0.9280 - val_loss: 0.6648 - val_acc: 0.7755\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.61037\n",
      "Epoch 6/200\n",
      " - 0s - loss: 0.1515 - acc: 0.9382 - val_loss: 0.8148 - val_acc: 0.7873\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.61037\n",
      "Epoch 7/200\n",
      " - 0s - loss: 0.1613 - acc: 0.9328 - val_loss: 0.7230 - val_acc: 0.8127\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.61037\n",
      "Epoch 8/200\n",
      " - 0s - loss: 0.1395 - acc: 0.9460 - val_loss: 0.6225 - val_acc: 0.7527\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.61037\n",
      "Epoch 9/200\n",
      " - 0s - loss: 0.1668 - acc: 0.9326 - val_loss: 0.7196 - val_acc: 0.8027\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.61037\n",
      "Epoch 10/200\n",
      " - 0s - loss: 0.1567 - acc: 0.9357 - val_loss: 0.6516 - val_acc: 0.7736\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.61037\n",
      "Epoch 11/200\n",
      " - 0s - loss: 0.1598 - acc: 0.9344 - val_loss: 0.7167 - val_acc: 0.7964\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.61037\n",
      "Epoch 12/200\n",
      " - 0s - loss: 0.1415 - acc: 0.9421 - val_loss: 0.6650 - val_acc: 0.8027\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.61037\n",
      "Epoch 13/200\n",
      " - 0s - loss: 0.1433 - acc: 0.9416 - val_loss: 0.5910 - val_acc: 0.8173\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.61037 to 0.59103, saving model to MLP_new.weights.best.hdf5\n",
      "Epoch 14/200\n",
      " - 0s - loss: 0.1324 - acc: 0.9459 - val_loss: 0.7571 - val_acc: 0.8073\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.59103\n",
      "Epoch 15/200\n",
      " - 0s - loss: 0.1498 - acc: 0.9403 - val_loss: 0.5338 - val_acc: 0.7945\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.59103 to 0.53384, saving model to MLP_new.weights.best.hdf5\n",
      "Epoch 16/200\n",
      " - 0s - loss: 0.1659 - acc: 0.9337 - val_loss: 0.6030 - val_acc: 0.7882\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.53384\n",
      "Epoch 17/200\n",
      " - 0s - loss: 0.1600 - acc: 0.9332 - val_loss: 0.6649 - val_acc: 0.7809\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.53384\n",
      "Epoch 18/200\n",
      " - 0s - loss: 0.1551 - acc: 0.9360 - val_loss: 0.6745 - val_acc: 0.8036\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.53384\n",
      "Epoch 19/200\n",
      " - 0s - loss: 0.1441 - acc: 0.9417 - val_loss: 0.6297 - val_acc: 0.8073\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.53384\n",
      "Epoch 20/200\n",
      " - 0s - loss: 0.1505 - acc: 0.9403 - val_loss: 0.6272 - val_acc: 0.8236\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.53384\n",
      "Epoch 21/200\n",
      " - 0s - loss: 0.1521 - acc: 0.9370 - val_loss: 0.6588 - val_acc: 0.8009\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.53384\n",
      "Epoch 22/200\n",
      " - 0s - loss: 0.1350 - acc: 0.9479 - val_loss: 0.8112 - val_acc: 0.7900\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.53384\n",
      "Epoch 23/200\n",
      " - 0s - loss: 0.1374 - acc: 0.9452 - val_loss: 0.7446 - val_acc: 0.8227\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.53384\n",
      "Epoch 24/200\n",
      " - 0s - loss: 0.1437 - acc: 0.9418 - val_loss: 0.5786 - val_acc: 0.8109\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.53384\n",
      "Epoch 25/200\n",
      " - 0s - loss: 0.1753 - acc: 0.9265 - val_loss: 0.8051 - val_acc: 0.7791\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.53384\n",
      "Epoch 26/200\n",
      " - 0s - loss: 0.1453 - acc: 0.9394 - val_loss: 0.6386 - val_acc: 0.8127\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.53384\n",
      "Epoch 27/200\n",
      " - 0s - loss: 0.1382 - acc: 0.9441 - val_loss: 0.6948 - val_acc: 0.8073\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.53384\n",
      "Epoch 28/200\n",
      " - 0s - loss: 0.1264 - acc: 0.9489 - val_loss: 0.6483 - val_acc: 0.8127\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.53384\n",
      "Epoch 29/200\n",
      " - 0s - loss: 0.1419 - acc: 0.9417 - val_loss: 0.8270 - val_acc: 0.8036\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.53384\n",
      "Epoch 30/200\n",
      " - 0s - loss: 0.1398 - acc: 0.9429 - val_loss: 0.7686 - val_acc: 0.8145\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.53384\n",
      "Epoch 31/200\n",
      " - 0s - loss: 0.1480 - acc: 0.9384 - val_loss: 0.6418 - val_acc: 0.8218\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.53384\n",
      "Epoch 32/200\n",
      " - 0s - loss: 0.1354 - acc: 0.9451 - val_loss: 0.8657 - val_acc: 0.7991\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.53384\n",
      "Epoch 33/200\n",
      " - 0s - loss: 0.1318 - acc: 0.9477 - val_loss: 0.7020 - val_acc: 0.8055\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.53384\n",
      "Epoch 34/200\n",
      " - 0s - loss: 0.1375 - acc: 0.9426 - val_loss: 1.0592 - val_acc: 0.7491\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.53384\n",
      "Epoch 35/200\n",
      " - 0s - loss: 0.1583 - acc: 0.9323 - val_loss: 0.7004 - val_acc: 0.8118\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.53384\n",
      "Epoch 36/200\n",
      " - 0s - loss: 0.1415 - acc: 0.9433 - val_loss: 0.6411 - val_acc: 0.8227\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.53384\n",
      "Epoch 37/200\n",
      " - 0s - loss: 0.1423 - acc: 0.9429 - val_loss: 0.5857 - val_acc: 0.8091\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.53384\n",
      "Epoch 38/200\n",
      " - 0s - loss: 0.1373 - acc: 0.9434 - val_loss: 0.5864 - val_acc: 0.8100\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.53384\n",
      "Epoch 39/200\n",
      " - 0s - loss: 0.1380 - acc: 0.9437 - val_loss: 0.5116 - val_acc: 0.8164\n",
      "\n",
      "Epoch 00039: val_loss improved from 0.53384 to 0.51165, saving model to MLP_new.weights.best.hdf5\n",
      "Epoch 40/200\n",
      " - 0s - loss: 0.1358 - acc: 0.9435 - val_loss: 0.6269 - val_acc: 0.8036\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.51165\n",
      "Epoch 41/200\n",
      " - 0s - loss: 0.1378 - acc: 0.9475 - val_loss: 0.7650 - val_acc: 0.7991\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.51165\n",
      "Epoch 42/200\n",
      " - 0s - loss: 0.1492 - acc: 0.9384 - val_loss: 0.5914 - val_acc: 0.8245\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.51165\n",
      "Epoch 43/200\n",
      " - 0s - loss: 0.1523 - acc: 0.9357 - val_loss: 0.6643 - val_acc: 0.7655\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.51165\n",
      "Epoch 44/200\n",
      " - 0s - loss: 0.1791 - acc: 0.9263 - val_loss: 0.5906 - val_acc: 0.8291\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.51165\n",
      "Epoch 45/200\n",
      " - 0s - loss: 0.1387 - acc: 0.9439 - val_loss: 0.6615 - val_acc: 0.8236\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.51165\n",
      "Epoch 46/200\n",
      " - 0s - loss: 0.1212 - acc: 0.9505 - val_loss: 0.6602 - val_acc: 0.8100\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.51165\n",
      "Epoch 47/200\n",
      " - 0s - loss: 0.1350 - acc: 0.9468 - val_loss: 0.5768 - val_acc: 0.8245\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.51165\n",
      "Epoch 48/200\n",
      " - 0s - loss: 0.1367 - acc: 0.9446 - val_loss: 0.6440 - val_acc: 0.7718\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.51165\n",
      "Epoch 49/200\n",
      " - 0s - loss: 0.1557 - acc: 0.9378 - val_loss: 0.6455 - val_acc: 0.7973\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.51165\n",
      "Epoch 50/200\n",
      " - 0s - loss: 0.1536 - acc: 0.9358 - val_loss: 0.7603 - val_acc: 0.7973\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.51165\n",
      "Epoch 51/200\n",
      " - 0s - loss: 0.1481 - acc: 0.9398 - val_loss: 0.6547 - val_acc: 0.8055\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.51165\n",
      "Epoch 52/200\n",
      " - 0s - loss: 0.1356 - acc: 0.9447 - val_loss: 0.6200 - val_acc: 0.7673\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.51165\n",
      "Epoch 53/200\n",
      " - 0s - loss: 0.1424 - acc: 0.9416 - val_loss: 0.5805 - val_acc: 0.8136\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.51165\n",
      "Epoch 54/200\n",
      " - 0s - loss: 0.1181 - acc: 0.9524 - val_loss: 0.5923 - val_acc: 0.7936\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.51165\n",
      "Epoch 55/200\n",
      " - 0s - loss: 0.1261 - acc: 0.9496 - val_loss: 0.6190 - val_acc: 0.8209\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.51165\n",
      "Epoch 56/200\n",
      " - 0s - loss: 0.1298 - acc: 0.9482 - val_loss: 0.7835 - val_acc: 0.8055\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.51165\n",
      "Epoch 57/200\n",
      " - 0s - loss: 0.1280 - acc: 0.9477 - val_loss: 0.6515 - val_acc: 0.8209\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.51165\n",
      "Epoch 58/200\n",
      " - 0s - loss: 0.1272 - acc: 0.9493 - val_loss: 0.6497 - val_acc: 0.8245\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.51165\n",
      "Epoch 59/200\n",
      " - 0s - loss: 0.1131 - acc: 0.9542 - val_loss: 0.7628 - val_acc: 0.8173\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.51165\n",
      "Epoch 60/200\n",
      " - 0s - loss: 0.1213 - acc: 0.9515 - val_loss: 0.6008 - val_acc: 0.8273\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.51165\n",
      "Epoch 61/200\n",
      " - 0s - loss: 0.1192 - acc: 0.9518 - val_loss: 0.7349 - val_acc: 0.8082\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.51165\n",
      "Epoch 62/200\n",
      " - 0s - loss: 0.1358 - acc: 0.9466 - val_loss: 0.6640 - val_acc: 0.7882\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.51165\n",
      "Epoch 63/200\n",
      " - 0s - loss: 0.1348 - acc: 0.9457 - val_loss: 0.5838 - val_acc: 0.8018\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.51165\n",
      "Epoch 64/200\n",
      " - 0s - loss: 0.1337 - acc: 0.9449 - val_loss: 0.7962 - val_acc: 0.8073\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 0.51165\n",
      "Epoch 65/200\n",
      " - 0s - loss: 0.1195 - acc: 0.9512 - val_loss: 0.7402 - val_acc: 0.8191\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 0.51165\n",
      "Epoch 66/200\n",
      " - 0s - loss: 0.1259 - acc: 0.9489 - val_loss: 0.6694 - val_acc: 0.8118\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.51165\n",
      "Epoch 67/200\n",
      " - 0s - loss: 0.1278 - acc: 0.9497 - val_loss: 0.7732 - val_acc: 0.7873\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.51165\n",
      "Epoch 68/200\n",
      " - 0s - loss: 0.1653 - acc: 0.9313 - val_loss: 0.5636 - val_acc: 0.7973\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.51165\n",
      "Epoch 69/200\n",
      " - 0s - loss: 0.1496 - acc: 0.9386 - val_loss: 0.7527 - val_acc: 0.8027\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.51165\n",
      "Epoch 70/200\n",
      " - 0s - loss: 0.1273 - acc: 0.9502 - val_loss: 0.6973 - val_acc: 0.8264\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.51165\n",
      "Epoch 71/200\n",
      " - 0s - loss: 0.1229 - acc: 0.9497 - val_loss: 0.6299 - val_acc: 0.8300\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.51165\n",
      "Epoch 72/200\n",
      " - 0s - loss: 0.1159 - acc: 0.9555 - val_loss: 0.5852 - val_acc: 0.8164\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.51165\n",
      "Epoch 73/200\n",
      " - 0s - loss: 0.1193 - acc: 0.9527 - val_loss: 0.7655 - val_acc: 0.8127\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 0.51165\n",
      "Epoch 74/200\n",
      " - 0s - loss: 0.1276 - acc: 0.9479 - val_loss: 0.6909 - val_acc: 0.8173\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 0.51165\n",
      "Epoch 75/200\n",
      " - 0s - loss: 0.1297 - acc: 0.9456 - val_loss: 0.7010 - val_acc: 0.8064\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 0.51165\n",
      "Epoch 76/200\n",
      " - 0s - loss: 0.1310 - acc: 0.9471 - val_loss: 0.7293 - val_acc: 0.8191\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 0.51165\n",
      "Epoch 77/200\n",
      " - 0s - loss: 0.1286 - acc: 0.9471 - val_loss: 0.8272 - val_acc: 0.8009\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 0.51165\n",
      "Epoch 78/200\n",
      " - 0s - loss: 0.1210 - acc: 0.9537 - val_loss: 0.6020 - val_acc: 0.8291\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 0.51165\n",
      "Epoch 79/200\n",
      " - 0s - loss: 0.1212 - acc: 0.9516 - val_loss: 0.8500 - val_acc: 0.8073\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00079: val_loss did not improve from 0.51165\n",
      "Epoch 80/200\n",
      " - 0s - loss: 0.1269 - acc: 0.9485 - val_loss: 0.6173 - val_acc: 0.8127\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 0.51165\n",
      "Epoch 81/200\n",
      " - 0s - loss: 0.1248 - acc: 0.9506 - val_loss: 0.6928 - val_acc: 0.8073\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 0.51165\n",
      "Epoch 82/200\n",
      " - 0s - loss: 0.1389 - acc: 0.9423 - val_loss: 0.6808 - val_acc: 0.8145\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 0.51165\n",
      "Epoch 83/200\n",
      " - 0s - loss: 0.1250 - acc: 0.9503 - val_loss: 0.6263 - val_acc: 0.8336\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 0.51165\n",
      "Epoch 84/200\n",
      " - 0s - loss: 0.1236 - acc: 0.9525 - val_loss: 0.5383 - val_acc: 0.8064\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 0.51165\n",
      "Epoch 85/200\n",
      " - 0s - loss: 0.1411 - acc: 0.9433 - val_loss: 0.6813 - val_acc: 0.8109\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 0.51165\n",
      "Epoch 86/200\n",
      " - 0s - loss: 0.1508 - acc: 0.9397 - val_loss: 0.5609 - val_acc: 0.8009\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 0.51165\n",
      "Epoch 87/200\n",
      " - 0s - loss: 0.1333 - acc: 0.9446 - val_loss: 0.6422 - val_acc: 0.8164\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 0.51165\n",
      "Epoch 88/200\n",
      " - 0s - loss: 0.1278 - acc: 0.9492 - val_loss: 0.5228 - val_acc: 0.8255\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 0.51165\n",
      "Epoch 89/200\n",
      " - 0s - loss: 0.1199 - acc: 0.9527 - val_loss: 0.6962 - val_acc: 0.8109\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 0.51165\n",
      "Epoch 90/200\n",
      " - 0s - loss: 0.1193 - acc: 0.9526 - val_loss: 0.6283 - val_acc: 0.8227\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 0.51165\n",
      "Epoch 91/200\n",
      " - 0s - loss: 0.1218 - acc: 0.9502 - val_loss: 0.6673 - val_acc: 0.8182\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 0.51165\n",
      "Epoch 92/200\n",
      " - 0s - loss: 0.1231 - acc: 0.9506 - val_loss: 0.6105 - val_acc: 0.8155\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 0.51165\n",
      "Epoch 93/200\n",
      " - 0s - loss: 0.1381 - acc: 0.9452 - val_loss: 0.7268 - val_acc: 0.8064\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 0.51165\n",
      "Epoch 94/200\n",
      " - 0s - loss: 0.1191 - acc: 0.9506 - val_loss: 0.6748 - val_acc: 0.8209\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 0.51165\n",
      "Epoch 95/200\n",
      " - 0s - loss: 0.1274 - acc: 0.9482 - val_loss: 0.6034 - val_acc: 0.8282\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 0.51165\n",
      "Epoch 96/200\n",
      " - 0s - loss: 0.1429 - acc: 0.9411 - val_loss: 0.6351 - val_acc: 0.8064\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 0.51165\n",
      "Epoch 97/200\n",
      " - 0s - loss: 0.1147 - acc: 0.9556 - val_loss: 0.7359 - val_acc: 0.8191\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 0.51165\n",
      "Epoch 98/200\n",
      " - 0s - loss: 0.1122 - acc: 0.9541 - val_loss: 0.6422 - val_acc: 0.7791\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 0.51165\n",
      "Epoch 99/200\n",
      " - 0s - loss: 0.1331 - acc: 0.9416 - val_loss: 0.7160 - val_acc: 0.8173\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 0.51165\n",
      "Epoch 100/200\n",
      " - 0s - loss: 0.1195 - acc: 0.9514 - val_loss: 0.5870 - val_acc: 0.7955\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 0.51165\n",
      "Epoch 101/200\n",
      " - 0s - loss: 0.1198 - acc: 0.9518 - val_loss: 0.6240 - val_acc: 0.8091\n",
      "\n",
      "Epoch 00101: val_loss did not improve from 0.51165\n",
      "Epoch 102/200\n",
      " - 0s - loss: 0.1283 - acc: 0.9470 - val_loss: 0.6187 - val_acc: 0.8173\n",
      "\n",
      "Epoch 00102: val_loss did not improve from 0.51165\n",
      "Epoch 103/200\n",
      " - 0s - loss: 0.1446 - acc: 0.9400 - val_loss: 0.5067 - val_acc: 0.8273\n",
      "\n",
      "Epoch 00103: val_loss improved from 0.51165 to 0.50667, saving model to MLP_new.weights.best.hdf5\n",
      "Epoch 104/200\n",
      " - 0s - loss: 0.1261 - acc: 0.9508 - val_loss: 0.7033 - val_acc: 0.8145\n",
      "\n",
      "Epoch 00104: val_loss did not improve from 0.50667\n",
      "Epoch 105/200\n",
      " - 0s - loss: 0.1311 - acc: 0.9470 - val_loss: 0.7869 - val_acc: 0.7955\n",
      "\n",
      "Epoch 00105: val_loss did not improve from 0.50667\n",
      "Epoch 106/200\n",
      " - 0s - loss: 0.1270 - acc: 0.9480 - val_loss: 0.7334 - val_acc: 0.8118\n",
      "\n",
      "Epoch 00106: val_loss did not improve from 0.50667\n",
      "Epoch 107/200\n",
      " - 0s - loss: 0.1228 - acc: 0.9491 - val_loss: 0.5684 - val_acc: 0.8309\n",
      "\n",
      "Epoch 00107: val_loss did not improve from 0.50667\n",
      "Epoch 108/200\n",
      " - 0s - loss: 0.1159 - acc: 0.9534 - val_loss: 0.5646 - val_acc: 0.8264\n",
      "\n",
      "Epoch 00108: val_loss did not improve from 0.50667\n",
      "Epoch 109/200\n",
      " - 0s - loss: 0.1283 - acc: 0.9473 - val_loss: 0.5979 - val_acc: 0.8282\n",
      "\n",
      "Epoch 00109: val_loss did not improve from 0.50667\n",
      "Epoch 110/200\n",
      " - 0s - loss: 0.1449 - acc: 0.9405 - val_loss: 0.5563 - val_acc: 0.7945\n",
      "\n",
      "Epoch 00110: val_loss did not improve from 0.50667\n",
      "Epoch 111/200\n",
      " - 0s - loss: 0.1416 - acc: 0.9437 - val_loss: 0.6251 - val_acc: 0.8155\n",
      "\n",
      "Epoch 00111: val_loss did not improve from 0.50667\n",
      "Epoch 112/200\n",
      " - 0s - loss: 0.1255 - acc: 0.9490 - val_loss: 0.6584 - val_acc: 0.8036\n",
      "\n",
      "Epoch 00112: val_loss did not improve from 0.50667\n",
      "Epoch 113/200\n",
      " - 0s - loss: 0.1266 - acc: 0.9504 - val_loss: 0.6309 - val_acc: 0.8209\n",
      "\n",
      "Epoch 00113: val_loss did not improve from 0.50667\n",
      "Epoch 114/200\n",
      " - 0s - loss: 0.1148 - acc: 0.9546 - val_loss: 0.5908 - val_acc: 0.8336\n",
      "\n",
      "Epoch 00114: val_loss did not improve from 0.50667\n",
      "Epoch 115/200\n",
      " - 0s - loss: 0.1109 - acc: 0.9556 - val_loss: 0.6575 - val_acc: 0.8064\n",
      "\n",
      "Epoch 00115: val_loss did not improve from 0.50667\n",
      "Epoch 116/200\n",
      " - 0s - loss: 0.1216 - acc: 0.9499 - val_loss: 0.6214 - val_acc: 0.8155\n",
      "\n",
      "Epoch 00116: val_loss did not improve from 0.50667\n",
      "Epoch 117/200\n",
      " - 0s - loss: 0.1246 - acc: 0.9482 - val_loss: 0.6561 - val_acc: 0.8018\n",
      "\n",
      "Epoch 00117: val_loss did not improve from 0.50667\n",
      "Epoch 118/200\n",
      " - 0s - loss: 0.1373 - acc: 0.9439 - val_loss: 0.6323 - val_acc: 0.8155\n",
      "\n",
      "Epoch 00118: val_loss did not improve from 0.50667\n",
      "Epoch 119/200\n",
      " - 0s - loss: 0.1228 - acc: 0.9500 - val_loss: 0.6384 - val_acc: 0.8345\n",
      "\n",
      "Epoch 00119: val_loss did not improve from 0.50667\n",
      "Epoch 120/200\n",
      " - 0s - loss: 0.1455 - acc: 0.9403 - val_loss: 0.6869 - val_acc: 0.8073\n",
      "\n",
      "Epoch 00120: val_loss did not improve from 0.50667\n",
      "Epoch 121/200\n",
      " - 0s - loss: 0.1225 - acc: 0.9499 - val_loss: 0.7531 - val_acc: 0.8127\n",
      "\n",
      "Epoch 00121: val_loss did not improve from 0.50667\n",
      "Epoch 122/200\n",
      " - 0s - loss: 0.1423 - acc: 0.9443 - val_loss: 0.5365 - val_acc: 0.7836\n",
      "\n",
      "Epoch 00122: val_loss did not improve from 0.50667\n",
      "Epoch 123/200\n",
      " - 0s - loss: 0.1580 - acc: 0.9316 - val_loss: 0.5266 - val_acc: 0.7827\n",
      "\n",
      "Epoch 00123: val_loss did not improve from 0.50667\n",
      "Epoch 124/200\n",
      " - 0s - loss: 0.1272 - acc: 0.9478 - val_loss: 0.6694 - val_acc: 0.8227\n",
      "\n",
      "Epoch 00124: val_loss did not improve from 0.50667\n",
      "Epoch 125/200\n",
      " - 0s - loss: 0.1221 - acc: 0.9513 - val_loss: 0.6589 - val_acc: 0.8245\n",
      "\n",
      "Epoch 00125: val_loss did not improve from 0.50667\n",
      "Epoch 126/200\n",
      " - 0s - loss: 0.1197 - acc: 0.9517 - val_loss: 0.6186 - val_acc: 0.8036\n",
      "\n",
      "Epoch 00126: val_loss did not improve from 0.50667\n",
      "Epoch 127/200\n",
      " - 0s - loss: 0.1245 - acc: 0.9510 - val_loss: 0.5520 - val_acc: 0.8082\n",
      "\n",
      "Epoch 00127: val_loss did not improve from 0.50667\n",
      "Epoch 128/200\n",
      " - 0s - loss: 0.1161 - acc: 0.9545 - val_loss: 0.6526 - val_acc: 0.8036\n",
      "\n",
      "Epoch 00128: val_loss did not improve from 0.50667\n",
      "Epoch 129/200\n",
      " - 0s - loss: 0.1221 - acc: 0.9492 - val_loss: 0.6272 - val_acc: 0.8273\n",
      "\n",
      "Epoch 00129: val_loss did not improve from 0.50667\n",
      "Epoch 130/200\n",
      " - 0s - loss: 0.1229 - acc: 0.9510 - val_loss: 0.6251 - val_acc: 0.8164\n",
      "\n",
      "Epoch 00130: val_loss did not improve from 0.50667\n",
      "Epoch 131/200\n",
      " - 0s - loss: 0.1050 - acc: 0.9599 - val_loss: 0.5247 - val_acc: 0.8109\n",
      "\n",
      "Epoch 00131: val_loss did not improve from 0.50667\n",
      "Epoch 132/200\n",
      " - 0s - loss: 0.1110 - acc: 0.9555 - val_loss: 0.5408 - val_acc: 0.8091\n",
      "\n",
      "Epoch 00132: val_loss did not improve from 0.50667\n",
      "Epoch 133/200\n",
      " - 0s - loss: 0.1238 - acc: 0.9492 - val_loss: 0.5274 - val_acc: 0.8255\n",
      "\n",
      "Epoch 00133: val_loss did not improve from 0.50667\n",
      "Epoch 134/200\n",
      " - 0s - loss: 0.1253 - acc: 0.9485 - val_loss: 0.5510 - val_acc: 0.8282\n",
      "\n",
      "Epoch 00134: val_loss did not improve from 0.50667\n",
      "Epoch 135/200\n",
      " - 0s - loss: 0.1626 - acc: 0.9307 - val_loss: 0.5619 - val_acc: 0.8264\n",
      "\n",
      "Epoch 00135: val_loss did not improve from 0.50667\n",
      "Epoch 136/200\n",
      " - 0s - loss: 0.1391 - acc: 0.9440 - val_loss: 0.6026 - val_acc: 0.8218\n",
      "\n",
      "Epoch 00136: val_loss did not improve from 0.50667\n",
      "Epoch 137/200\n",
      " - 0s - loss: 0.1270 - acc: 0.9474 - val_loss: 0.5389 - val_acc: 0.8255\n",
      "\n",
      "Epoch 00137: val_loss did not improve from 0.50667\n",
      "Epoch 138/200\n",
      " - 0s - loss: 0.1144 - acc: 0.9545 - val_loss: 0.5989 - val_acc: 0.8236\n",
      "\n",
      "Epoch 00138: val_loss did not improve from 0.50667\n",
      "Epoch 139/200\n",
      " - 0s - loss: 0.1045 - acc: 0.9580 - val_loss: 0.6086 - val_acc: 0.8227\n",
      "\n",
      "Epoch 00139: val_loss did not improve from 0.50667\n",
      "Epoch 140/200\n",
      " - 0s - loss: 0.1099 - acc: 0.9562 - val_loss: 0.5690 - val_acc: 0.8327\n",
      "\n",
      "Epoch 00140: val_loss did not improve from 0.50667\n",
      "Epoch 141/200\n",
      " - 0s - loss: 0.1093 - acc: 0.9564 - val_loss: 0.5462 - val_acc: 0.8391\n",
      "\n",
      "Epoch 00141: val_loss did not improve from 0.50667\n",
      "Epoch 142/200\n",
      " - 0s - loss: 0.1081 - acc: 0.9584 - val_loss: 0.6339 - val_acc: 0.8182\n",
      "\n",
      "Epoch 00142: val_loss did not improve from 0.50667\n",
      "Epoch 143/200\n",
      " - 0s - loss: 0.1155 - acc: 0.9537 - val_loss: 0.5414 - val_acc: 0.8082\n",
      "\n",
      "Epoch 00143: val_loss did not improve from 0.50667\n",
      "Epoch 144/200\n",
      " - 0s - loss: 0.1191 - acc: 0.9519 - val_loss: 0.5851 - val_acc: 0.8273\n",
      "\n",
      "Epoch 00144: val_loss did not improve from 0.50667\n",
      "Epoch 145/200\n",
      " - 0s - loss: 0.1113 - acc: 0.9558 - val_loss: 0.4912 - val_acc: 0.8200\n",
      "\n",
      "Epoch 00145: val_loss improved from 0.50667 to 0.49123, saving model to MLP_new.weights.best.hdf5\n",
      "Epoch 146/200\n",
      " - 0s - loss: 0.1107 - acc: 0.9556 - val_loss: 0.5383 - val_acc: 0.7918\n",
      "\n",
      "Epoch 00146: val_loss did not improve from 0.49123\n",
      "Epoch 147/200\n",
      " - 0s - loss: 0.1143 - acc: 0.9557 - val_loss: 0.6420 - val_acc: 0.8273\n",
      "\n",
      "Epoch 00147: val_loss did not improve from 0.49123\n",
      "Epoch 148/200\n",
      " - 0s - loss: 0.1121 - acc: 0.9552 - val_loss: 0.6166 - val_acc: 0.8309\n",
      "\n",
      "Epoch 00148: val_loss did not improve from 0.49123\n",
      "Epoch 149/200\n",
      " - 0s - loss: 0.1036 - acc: 0.9574 - val_loss: 0.4957 - val_acc: 0.8436\n",
      "\n",
      "Epoch 00149: val_loss did not improve from 0.49123\n",
      "Epoch 150/200\n",
      " - 0s - loss: 0.1139 - acc: 0.9552 - val_loss: 0.5686 - val_acc: 0.8382\n",
      "\n",
      "Epoch 00150: val_loss did not improve from 0.49123\n",
      "Epoch 151/200\n",
      " - 0s - loss: 0.1075 - acc: 0.9576 - val_loss: 0.5015 - val_acc: 0.8400\n",
      "\n",
      "Epoch 00151: val_loss did not improve from 0.49123\n",
      "Epoch 152/200\n",
      " - 0s - loss: 0.1096 - acc: 0.9562 - val_loss: 0.6256 - val_acc: 0.8173\n",
      "\n",
      "Epoch 00152: val_loss did not improve from 0.49123\n",
      "Epoch 153/200\n",
      " - 0s - loss: 0.1141 - acc: 0.9542 - val_loss: 0.5186 - val_acc: 0.8382\n",
      "\n",
      "Epoch 00153: val_loss did not improve from 0.49123\n",
      "Epoch 154/200\n",
      " - 0s - loss: 0.1188 - acc: 0.9538 - val_loss: 0.5452 - val_acc: 0.8245\n",
      "\n",
      "Epoch 00154: val_loss did not improve from 0.49123\n",
      "Epoch 155/200\n",
      " - 0s - loss: 0.1372 - acc: 0.9444 - val_loss: 0.5729 - val_acc: 0.8145\n",
      "\n",
      "Epoch 00155: val_loss did not improve from 0.49123\n",
      "Epoch 156/200\n",
      " - 0s - loss: 0.1583 - acc: 0.9357 - val_loss: 0.6064 - val_acc: 0.8073\n",
      "\n",
      "Epoch 00156: val_loss did not improve from 0.49123\n",
      "Epoch 157/200\n",
      " - 0s - loss: 0.1317 - acc: 0.9483 - val_loss: 0.5769 - val_acc: 0.8182\n",
      "\n",
      "Epoch 00157: val_loss did not improve from 0.49123\n",
      "Epoch 158/200\n",
      " - 0s - loss: 0.1215 - acc: 0.9519 - val_loss: 0.5979 - val_acc: 0.8200\n",
      "\n",
      "Epoch 00158: val_loss did not improve from 0.49123\n",
      "Epoch 159/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 0s - loss: 0.1099 - acc: 0.9554 - val_loss: 0.5980 - val_acc: 0.8364\n",
      "\n",
      "Epoch 00159: val_loss did not improve from 0.49123\n",
      "Epoch 160/200\n",
      " - 0s - loss: 0.1271 - acc: 0.9498 - val_loss: 0.5861 - val_acc: 0.8164\n",
      "\n",
      "Epoch 00160: val_loss did not improve from 0.49123\n",
      "Epoch 161/200\n",
      " - 0s - loss: 0.1200 - acc: 0.9518 - val_loss: 0.5700 - val_acc: 0.8318\n",
      "\n",
      "Epoch 00161: val_loss did not improve from 0.49123\n",
      "Epoch 162/200\n",
      " - 0s - loss: 0.1234 - acc: 0.9498 - val_loss: 0.6797 - val_acc: 0.8173\n",
      "\n",
      "Epoch 00162: val_loss did not improve from 0.49123\n",
      "Epoch 163/200\n",
      " - 0s - loss: 0.1145 - acc: 0.9549 - val_loss: 0.5748 - val_acc: 0.8155\n",
      "\n",
      "Epoch 00163: val_loss did not improve from 0.49123\n",
      "Epoch 164/200\n",
      " - 0s - loss: 0.1259 - acc: 0.9477 - val_loss: 0.5544 - val_acc: 0.8400\n",
      "\n",
      "Epoch 00164: val_loss did not improve from 0.49123\n",
      "Epoch 165/200\n",
      " - 0s - loss: 0.1463 - acc: 0.9418 - val_loss: 0.6388 - val_acc: 0.8018\n",
      "\n",
      "Epoch 00165: val_loss did not improve from 0.49123\n",
      "Epoch 166/200\n",
      " - 0s - loss: 0.1426 - acc: 0.9420 - val_loss: 0.6367 - val_acc: 0.8218\n",
      "\n",
      "Epoch 00166: val_loss did not improve from 0.49123\n",
      "Epoch 167/200\n",
      " - 0s - loss: 0.1343 - acc: 0.9443 - val_loss: 0.6101 - val_acc: 0.8082\n",
      "\n",
      "Epoch 00167: val_loss did not improve from 0.49123\n",
      "Epoch 168/200\n",
      " - 0s - loss: 0.1288 - acc: 0.9479 - val_loss: 0.5072 - val_acc: 0.7891\n",
      "\n",
      "Epoch 00168: val_loss did not improve from 0.49123\n",
      "Epoch 169/200\n",
      " - 0s - loss: 0.1207 - acc: 0.9501 - val_loss: 0.6277 - val_acc: 0.8264\n",
      "\n",
      "Epoch 00169: val_loss did not improve from 0.49123\n",
      "Epoch 170/200\n",
      " - 0s - loss: 0.1076 - acc: 0.9566 - val_loss: 0.6373 - val_acc: 0.8327\n",
      "\n",
      "Epoch 00170: val_loss did not improve from 0.49123\n",
      "Epoch 171/200\n",
      " - 0s - loss: 0.1094 - acc: 0.9554 - val_loss: 0.6263 - val_acc: 0.8327\n",
      "\n",
      "Epoch 00171: val_loss did not improve from 0.49123\n",
      "Epoch 172/200\n",
      " - 0s - loss: 0.1233 - acc: 0.9511 - val_loss: 0.5700 - val_acc: 0.8355\n",
      "\n",
      "Epoch 00172: val_loss did not improve from 0.49123\n",
      "Epoch 173/200\n",
      " - 0s - loss: 0.1113 - acc: 0.9547 - val_loss: 0.5486 - val_acc: 0.8364\n",
      "\n",
      "Epoch 00173: val_loss did not improve from 0.49123\n",
      "Epoch 174/200\n",
      " - 0s - loss: 0.1102 - acc: 0.9533 - val_loss: 0.5793 - val_acc: 0.8064\n",
      "\n",
      "Epoch 00174: val_loss did not improve from 0.49123\n",
      "Epoch 175/200\n",
      " - 0s - loss: 0.1130 - acc: 0.9528 - val_loss: 0.6197 - val_acc: 0.8309\n",
      "\n",
      "Epoch 00175: val_loss did not improve from 0.49123\n",
      "Epoch 176/200\n",
      " - 0s - loss: 0.1062 - acc: 0.9570 - val_loss: 0.7528 - val_acc: 0.8091\n",
      "\n",
      "Epoch 00176: val_loss did not improve from 0.49123\n",
      "Epoch 177/200\n",
      " - 0s - loss: 0.1148 - acc: 0.9541 - val_loss: 0.6431 - val_acc: 0.8255\n",
      "\n",
      "Epoch 00177: val_loss did not improve from 0.49123\n",
      "Epoch 178/200\n",
      " - 0s - loss: 0.1137 - acc: 0.9560 - val_loss: 0.5440 - val_acc: 0.8264\n",
      "\n",
      "Epoch 00178: val_loss did not improve from 0.49123\n",
      "Epoch 179/200\n",
      " - 0s - loss: 0.1261 - acc: 0.9488 - val_loss: 0.5419 - val_acc: 0.8227\n",
      "\n",
      "Epoch 00179: val_loss did not improve from 0.49123\n",
      "Epoch 180/200\n",
      " - 0s - loss: 0.1158 - acc: 0.9542 - val_loss: 0.6303 - val_acc: 0.8155\n",
      "\n",
      "Epoch 00180: val_loss did not improve from 0.49123\n",
      "Epoch 181/200\n",
      " - 0s - loss: 0.1169 - acc: 0.9520 - val_loss: 0.4940 - val_acc: 0.8355\n",
      "\n",
      "Epoch 00181: val_loss did not improve from 0.49123\n",
      "Epoch 182/200\n",
      " - 0s - loss: 0.0989 - acc: 0.9602 - val_loss: 0.5594 - val_acc: 0.8182\n",
      "\n",
      "Epoch 00182: val_loss did not improve from 0.49123\n",
      "Epoch 183/200\n",
      " - 0s - loss: 0.1145 - acc: 0.9527 - val_loss: 0.5970 - val_acc: 0.8255\n",
      "\n",
      "Epoch 00183: val_loss did not improve from 0.49123\n",
      "Epoch 184/200\n",
      " - 0s - loss: 0.1176 - acc: 0.9516 - val_loss: 0.5513 - val_acc: 0.7545\n",
      "\n",
      "Epoch 00184: val_loss did not improve from 0.49123\n",
      "Epoch 185/200\n",
      " - 0s - loss: 0.1430 - acc: 0.9403 - val_loss: 0.7781 - val_acc: 0.7927\n",
      "\n",
      "Epoch 00185: val_loss did not improve from 0.49123\n",
      "Epoch 186/200\n",
      " - 0s - loss: 0.1238 - acc: 0.9485 - val_loss: 0.6503 - val_acc: 0.8091\n",
      "\n",
      "Epoch 00186: val_loss did not improve from 0.49123\n",
      "Epoch 187/200\n",
      " - 0s - loss: 0.1010 - acc: 0.9603 - val_loss: 0.5646 - val_acc: 0.8436\n",
      "\n",
      "Epoch 00187: val_loss did not improve from 0.49123\n",
      "Epoch 188/200\n",
      " - 0s - loss: 0.1067 - acc: 0.9575 - val_loss: 0.5534 - val_acc: 0.8364\n",
      "\n",
      "Epoch 00188: val_loss did not improve from 0.49123\n",
      "Epoch 189/200\n",
      " - 0s - loss: 0.0975 - acc: 0.9609 - val_loss: 0.5303 - val_acc: 0.8409\n",
      "\n",
      "Epoch 00189: val_loss did not improve from 0.49123\n",
      "Epoch 190/200\n",
      " - 0s - loss: 0.1006 - acc: 0.9605 - val_loss: 0.5592 - val_acc: 0.8064\n",
      "\n",
      "Epoch 00190: val_loss did not improve from 0.49123\n",
      "Epoch 191/200\n",
      " - 0s - loss: 0.1164 - acc: 0.9525 - val_loss: 0.5040 - val_acc: 0.8164\n",
      "\n",
      "Epoch 00191: val_loss did not improve from 0.49123\n",
      "Epoch 192/200\n",
      " - 0s - loss: 0.1075 - acc: 0.9595 - val_loss: 0.6301 - val_acc: 0.8327\n",
      "\n",
      "Epoch 00192: val_loss did not improve from 0.49123\n",
      "Epoch 193/200\n",
      " - 0s - loss: 0.1177 - acc: 0.9545 - val_loss: 0.7446 - val_acc: 0.7991\n",
      "\n",
      "Epoch 00193: val_loss did not improve from 0.49123\n",
      "Epoch 194/200\n",
      " - 0s - loss: 0.1122 - acc: 0.9554 - val_loss: 0.5637 - val_acc: 0.7973\n",
      "\n",
      "Epoch 00194: val_loss did not improve from 0.49123\n",
      "Epoch 195/200\n",
      " - 0s - loss: 0.1241 - acc: 0.9476 - val_loss: 0.5508 - val_acc: 0.8127\n",
      "\n",
      "Epoch 00195: val_loss did not improve from 0.49123\n",
      "Epoch 196/200\n",
      " - 0s - loss: 0.1544 - acc: 0.9368 - val_loss: 0.6133 - val_acc: 0.7900\n",
      "\n",
      "Epoch 00196: val_loss did not improve from 0.49123\n",
      "Epoch 197/200\n",
      " - 0s - loss: 0.1452 - acc: 0.9407 - val_loss: 0.4940 - val_acc: 0.8327\n",
      "\n",
      "Epoch 00197: val_loss did not improve from 0.49123\n",
      "Epoch 198/200\n",
      " - 0s - loss: 0.1132 - acc: 0.9547 - val_loss: 0.6205 - val_acc: 0.8264\n",
      "\n",
      "Epoch 00198: val_loss did not improve from 0.49123\n",
      "Epoch 199/200\n",
      " - 0s - loss: 0.1074 - acc: 0.9577 - val_loss: 0.5766 - val_acc: 0.8164\n",
      "\n",
      "Epoch 00199: val_loss did not improve from 0.49123\n",
      "Epoch 200/200\n",
      " - 0s - loss: 0.1150 - acc: 0.9543 - val_loss: 0.5437 - val_acc: 0.8309\n",
      "\n",
      "Epoch 00200: val_loss did not improve from 0.49123\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath = 'MLP_new.weights.best.hdf5', verbose = 1, save_best_only = True)\n",
    "hist1 = model2.fit( x_train, y_train, epochs = 200, batch_size=512, validation_split = 0.1, callbacks = [checkpointer], verbose = 2, shuffle = True )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1036,
     "status": "ok",
     "timestamp": 1528838562560,
     "user": {
      "displayName": "SHUBHAM VERMA",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "108363614808362191748"
     },
     "user_tz": -330
    },
    "id": "aNKmoGdQchbo",
    "outputId": "ae555b2c-cba8-4e72-cc55-5614e8b99c9c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3980/3980 [==============================] - 0s 77us/step\n",
      "Accuracy:  0.7557788944124576\n"
     ]
    }
   ],
   "source": [
    "score = model2.evaluate(x_test, y_test, verbose=1)\n",
    "print(\"Accuracy: \", score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "elVEGDtW3JfM"
   },
   "outputs": [],
   "source": [
    "predict3 = [1 if a>=0.5 else 0 for a in model2.predict(x_test)] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 91
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1086,
     "status": "ok",
     "timestamp": 1528839774224,
     "user": {
      "displayName": "SHUBHAM VERMA",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "108363614808362191748"
     },
     "user_tz": -330
    },
    "id": "bww5JR0i3PGh",
    "outputId": "6b0b172d-8479-4248-e644-a66fa315f6d3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.7557788944723618\n",
      "Precision = 0.7142857142857143\n",
      "Recall = 0.8642892521050025\n",
      "F1 Score = 0.7821604661586732\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy = {}\\nPrecision = {}\\nRecall = {}\\nF1 Score = {}\".format(metrics.accuracy_score(y_test, predict3), metrics.precision_score(y_test, predict3),metrics.recall_score(y_test, predict3),metrics.f1_score(y_test, predict3)))\n",
    "score_p.append([metrics.accuracy_score(y_test, predict3), metrics.precision_score(y_test, predict3),metrics.recall_score(y_test, predict3),metrics.f1_score(y_test, predict3)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "PocmBpI__yv7"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lghx16y4_r8Q"
   },
   "source": [
    "# Training on  LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wNp083VrV_ai"
   },
   "source": [
    "### Creating 3D array dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "3AXLNcwEwbIx"
   },
   "outputs": [],
   "source": [
    "X_train = np.asarray(np.reshape(x_train, (x_train.shape[0], 14, 1)))\n",
    "X_test = np.asarray(np.reshape(x_test, (x_test.shape[0], 14, 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "Krv-k18n3mXy"
   },
   "outputs": [],
   "source": [
    "from keras.layers import Dense, Dropout, CuDNNLSTM, Embedding, LSTM\n",
    "from keras.models import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "DR_-hoc-_qx6"
   },
   "outputs": [],
   "source": [
    "model3 = Sequential()\n",
    "model3.add(LSTM(256,input_shape=(14, 1), return_sequences=True))\n",
    "model3.add(LSTM(256))\n",
    "model3.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 258
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1438,
     "status": "ok",
     "timestamp": 1528838677662,
     "user": {
      "displayName": "SHUBHAM VERMA",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "108363614808362191748"
     },
     "user_tz": -330
    },
    "id": "i8emSgOt_q1c",
    "outputId": "10afeef3-7634-456c-b363-7f6fa8e090c7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_7 (LSTM)                (None, 14, 256)           264192    \n",
      "_________________________________________________________________\n",
      "lstm_8 (LSTM)                (None, 256)               525312    \n",
      "_________________________________________________________________\n",
      "dense_45 (Dense)             (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 789,761\n",
      "Trainable params: 789,761\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model3.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model3.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 7482
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 276963,
     "status": "ok",
     "timestamp": 1528838958417,
     "user": {
      "displayName": "SHUBHAM VERMA",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "108363614808362191748"
     },
     "user_tz": -330
    },
    "id": "xBzDfMmU_q9s",
    "outputId": "28aab7f6-7404-49ae-b27f-f052677bf381"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9900 samples, validate on 1100 samples\n",
      "Epoch 1/100\n",
      " - 4s - loss: 0.6925 - acc: 0.5182 - val_loss: 0.6918 - val_acc: 0.5255\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.69181, saving model to LSTM.weights.best.hdf5\n",
      "Epoch 2/100\n",
      " - 3s - loss: 0.6914 - acc: 0.5245 - val_loss: 0.6910 - val_acc: 0.5255\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.69181 to 0.69103, saving model to LSTM.weights.best.hdf5\n",
      "Epoch 3/100\n",
      " - 3s - loss: 0.6887 - acc: 0.5283 - val_loss: 0.6900 - val_acc: 0.5300\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.69103 to 0.68997, saving model to LSTM.weights.best.hdf5\n",
      "Epoch 4/100\n",
      " - 3s - loss: 0.6791 - acc: 0.5585 - val_loss: 0.6790 - val_acc: 0.5609\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.68997 to 0.67901, saving model to LSTM.weights.best.hdf5\n",
      "Epoch 5/100\n",
      " - 3s - loss: 0.6741 - acc: 0.5611 - val_loss: 0.6844 - val_acc: 0.5518\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.67901\n",
      "Epoch 6/100\n",
      " - 3s - loss: 0.6728 - acc: 0.5595 - val_loss: 0.6815 - val_acc: 0.5682\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.67901\n",
      "Epoch 7/100\n",
      " - 3s - loss: 0.6571 - acc: 0.5789 - val_loss: 0.6676 - val_acc: 0.5882\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.67901 to 0.66757, saving model to LSTM.weights.best.hdf5\n",
      "Epoch 8/100\n",
      " - 3s - loss: 0.8433 - acc: 0.5404 - val_loss: 0.6910 - val_acc: 0.5273\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.66757\n",
      "Epoch 9/100\n",
      " - 3s - loss: 0.6911 - acc: 0.5328 - val_loss: 0.6902 - val_acc: 0.5273\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.66757\n",
      "Epoch 10/100\n",
      " - 3s - loss: 0.6890 - acc: 0.5445 - val_loss: 0.6896 - val_acc: 0.5345\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.66757\n",
      "Epoch 11/100\n",
      " - 3s - loss: 0.6878 - acc: 0.5292 - val_loss: 0.6894 - val_acc: 0.5282\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.66757\n",
      "Epoch 12/100\n",
      " - 3s - loss: 0.6875 - acc: 0.5524 - val_loss: 0.6893 - val_acc: 0.5409\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.66757\n",
      "Epoch 13/100\n",
      " - 3s - loss: 0.6865 - acc: 0.5397 - val_loss: 0.6908 - val_acc: 0.5145\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.66757\n",
      "Epoch 14/100\n",
      " - 3s - loss: 0.6862 - acc: 0.5491 - val_loss: 0.6886 - val_acc: 0.5391\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.66757\n",
      "Epoch 15/100\n",
      " - 3s - loss: 0.6847 - acc: 0.5314 - val_loss: 0.6874 - val_acc: 0.5309\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.66757\n",
      "Epoch 16/100\n",
      " - 3s - loss: 0.6837 - acc: 0.5498 - val_loss: 0.6874 - val_acc: 0.5427\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.66757\n",
      "Epoch 17/100\n",
      " - 3s - loss: 0.6828 - acc: 0.5465 - val_loss: 0.6856 - val_acc: 0.5227\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.66757\n",
      "Epoch 18/100\n",
      " - 3s - loss: 0.6784 - acc: 0.5518 - val_loss: 0.6881 - val_acc: 0.5373\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.66757\n",
      "Epoch 19/100\n",
      " - 3s - loss: 0.6756 - acc: 0.5576 - val_loss: 0.6834 - val_acc: 0.5436\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.66757\n",
      "Epoch 20/100\n",
      " - 3s - loss: 0.6753 - acc: 0.5527 - val_loss: 0.6828 - val_acc: 0.5427\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.66757\n",
      "Epoch 21/100\n",
      " - 3s - loss: 0.6633 - acc: 0.5728 - val_loss: 0.6796 - val_acc: 0.5536\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.66757\n",
      "Epoch 22/100\n",
      " - 3s - loss: 0.6642 - acc: 0.5692 - val_loss: 0.6851 - val_acc: 0.5164\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.66757\n",
      "Epoch 23/100\n",
      " - 3s - loss: 0.6618 - acc: 0.5820 - val_loss: 0.6775 - val_acc: 0.5418\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.66757\n",
      "Epoch 24/100\n",
      " - 3s - loss: 0.6411 - acc: 0.6081 - val_loss: 0.6972 - val_acc: 0.5109\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.66757\n",
      "Epoch 25/100\n",
      " - 3s - loss: 0.6429 - acc: 0.6028 - val_loss: 0.6728 - val_acc: 0.5545\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.66757\n",
      "Epoch 26/100\n",
      " - 3s - loss: 0.6297 - acc: 0.6147 - val_loss: 0.6747 - val_acc: 0.5436\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.66757\n",
      "Epoch 27/100\n",
      " - 3s - loss: 0.6297 - acc: 0.6177 - val_loss: 0.6911 - val_acc: 0.5845\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.66757\n",
      "Epoch 28/100\n",
      " - 3s - loss: 0.6213 - acc: 0.6198 - val_loss: 0.7163 - val_acc: 0.5473\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.66757\n",
      "Epoch 29/100\n",
      " - 3s - loss: 0.6165 - acc: 0.6286 - val_loss: 0.6626 - val_acc: 0.5918\n",
      "\n",
      "Epoch 00029: val_loss improved from 0.66757 to 0.66263, saving model to LSTM.weights.best.hdf5\n",
      "Epoch 30/100\n",
      " - 3s - loss: 0.6215 - acc: 0.6090 - val_loss: 0.6960 - val_acc: 0.5464\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.66263\n",
      "Epoch 31/100\n",
      " - 3s - loss: 0.6166 - acc: 0.6094 - val_loss: 0.6901 - val_acc: 0.5736\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.66263\n",
      "Epoch 32/100\n",
      " - 3s - loss: 0.6123 - acc: 0.6221 - val_loss: 0.6624 - val_acc: 0.5600\n",
      "\n",
      "Epoch 00032: val_loss improved from 0.66263 to 0.66238, saving model to LSTM.weights.best.hdf5\n",
      "Epoch 33/100\n",
      " - 3s - loss: 0.6100 - acc: 0.6079 - val_loss: 0.6779 - val_acc: 0.5455\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.66238\n",
      "Epoch 34/100\n",
      " - 3s - loss: 0.5987 - acc: 0.6339 - val_loss: 0.6685 - val_acc: 0.5509\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.66238\n",
      "Epoch 35/100\n",
      " - 3s - loss: 0.5999 - acc: 0.6372 - val_loss: 0.6730 - val_acc: 0.5345\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.66238\n",
      "Epoch 36/100\n",
      " - 3s - loss: 0.5872 - acc: 0.6507 - val_loss: 0.6743 - val_acc: 0.5427\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.66238\n",
      "Epoch 37/100\n",
      " - 3s - loss: 0.6092 - acc: 0.6256 - val_loss: 0.6546 - val_acc: 0.5691\n",
      "\n",
      "Epoch 00037: val_loss improved from 0.66238 to 0.65457, saving model to LSTM.weights.best.hdf5\n",
      "Epoch 38/100\n",
      " - 3s - loss: 0.5958 - acc: 0.6387 - val_loss: 0.6882 - val_acc: 0.5464\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.65457\n",
      "Epoch 39/100\n",
      " - 3s - loss: 0.5729 - acc: 0.6742 - val_loss: 0.6784 - val_acc: 0.5645\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.65457\n",
      "Epoch 40/100\n",
      " - 3s - loss: 0.5793 - acc: 0.6590 - val_loss: 0.6706 - val_acc: 0.5473\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.65457\n",
      "Epoch 41/100\n",
      " - 3s - loss: 0.6001 - acc: 0.6367 - val_loss: 0.6666 - val_acc: 0.5609\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.65457\n",
      "Epoch 42/100\n",
      " - 3s - loss: 0.5944 - acc: 0.6427 - val_loss: 0.6810 - val_acc: 0.5564\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.65457\n",
      "Epoch 43/100\n",
      " - 3s - loss: 0.5756 - acc: 0.6644 - val_loss: 0.6766 - val_acc: 0.5718\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.65457\n",
      "Epoch 44/100\n",
      " - 3s - loss: 0.5740 - acc: 0.6706 - val_loss: 0.6834 - val_acc: 0.5091\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.65457\n",
      "Epoch 45/100\n",
      " - 3s - loss: 0.6464 - acc: 0.5859 - val_loss: 0.6537 - val_acc: 0.5936\n",
      "\n",
      "Epoch 00045: val_loss improved from 0.65457 to 0.65373, saving model to LSTM.weights.best.hdf5\n",
      "Epoch 46/100\n",
      " - 3s - loss: 0.6122 - acc: 0.6061 - val_loss: 0.6576 - val_acc: 0.5427\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.65373\n",
      "Epoch 47/100\n",
      " - 3s - loss: 0.6066 - acc: 0.6133 - val_loss: 0.6620 - val_acc: 0.5509\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.65373\n",
      "Epoch 48/100\n",
      " - 3s - loss: 0.6089 - acc: 0.6188 - val_loss: 0.6635 - val_acc: 0.5455\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.65373\n",
      "Epoch 49/100\n",
      " - 3s - loss: 0.5859 - acc: 0.6451 - val_loss: 0.6857 - val_acc: 0.5436\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.65373\n",
      "Epoch 50/100\n",
      " - 3s - loss: 0.5739 - acc: 0.6620 - val_loss: 0.6777 - val_acc: 0.5500\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.65373\n",
      "Epoch 51/100\n",
      " - 3s - loss: 0.6082 - acc: 0.6173 - val_loss: 0.6594 - val_acc: 0.5391\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.65373\n",
      "Epoch 52/100\n",
      " - 3s - loss: 0.5882 - acc: 0.6423 - val_loss: 0.6560 - val_acc: 0.5509\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.65373\n",
      "Epoch 53/100\n",
      " - 3s - loss: 0.5786 - acc: 0.6553 - val_loss: 0.6969 - val_acc: 0.5236\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.65373\n",
      "Epoch 54/100\n",
      " - 3s - loss: 0.5685 - acc: 0.6681 - val_loss: 0.6688 - val_acc: 0.5400\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.65373\n",
      "Epoch 55/100\n",
      " - 3s - loss: 0.5816 - acc: 0.6478 - val_loss: 0.7042 - val_acc: 0.5355\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.65373\n",
      "Epoch 56/100\n",
      " - 3s - loss: 0.5586 - acc: 0.6865 - val_loss: 0.6739 - val_acc: 0.5609\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.65373\n",
      "Epoch 57/100\n",
      " - 3s - loss: 0.6073 - acc: 0.6309 - val_loss: 0.6980 - val_acc: 0.5355\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.65373\n",
      "Epoch 58/100\n",
      " - 3s - loss: 0.5569 - acc: 0.6809 - val_loss: 0.7085 - val_acc: 0.5682\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.65373\n",
      "Epoch 59/100\n",
      " - 3s - loss: 0.5341 - acc: 0.7065 - val_loss: 0.7484 - val_acc: 0.5545\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.65373\n",
      "Epoch 60/100\n",
      " - 3s - loss: 0.6086 - acc: 0.6279 - val_loss: 0.6686 - val_acc: 0.5536\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.65373\n",
      "Epoch 61/100\n",
      " - 3s - loss: 0.5830 - acc: 0.6442 - val_loss: 0.6907 - val_acc: 0.5264\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.65373\n",
      "Epoch 62/100\n",
      " - 3s - loss: 0.5885 - acc: 0.6552 - val_loss: 0.7001 - val_acc: 0.5455\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.65373\n",
      "Epoch 63/100\n",
      " - 3s - loss: 0.5275 - acc: 0.7167 - val_loss: 0.6933 - val_acc: 0.6009\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.65373\n",
      "Epoch 64/100\n",
      " - 3s - loss: 0.5830 - acc: 0.6582 - val_loss: 0.6658 - val_acc: 0.5636\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 0.65373\n",
      "Epoch 65/100\n",
      " - 3s - loss: 0.5476 - acc: 0.6968 - val_loss: 0.6938 - val_acc: 0.5827\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 0.65373\n",
      "Epoch 66/100\n",
      " - 3s - loss: 0.5378 - acc: 0.7062 - val_loss: 0.6988 - val_acc: 0.5782\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.65373\n",
      "Epoch 67/100\n",
      " - 3s - loss: 0.4919 - acc: 0.7451 - val_loss: 0.6996 - val_acc: 0.5864\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.65373\n",
      "Epoch 68/100\n",
      " - 3s - loss: 0.5116 - acc: 0.7272 - val_loss: 0.7083 - val_acc: 0.5882\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.65373\n",
      "Epoch 69/100\n",
      " - 3s - loss: 0.5036 - acc: 0.7377 - val_loss: 0.7391 - val_acc: 0.6000\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.65373\n",
      "Epoch 70/100\n",
      " - 3s - loss: 0.5129 - acc: 0.7274 - val_loss: 0.7044 - val_acc: 0.5891\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.65373\n",
      "Epoch 71/100\n",
      " - 3s - loss: 0.4905 - acc: 0.7487 - val_loss: 0.7152 - val_acc: 0.6055\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.65373\n",
      "Epoch 72/100\n",
      " - 3s - loss: 0.4807 - acc: 0.7555 - val_loss: 0.7195 - val_acc: 0.5900\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.65373\n",
      "Epoch 73/100\n",
      " - 3s - loss: 0.4677 - acc: 0.7653 - val_loss: 0.7282 - val_acc: 0.6091\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 0.65373\n",
      "Epoch 74/100\n",
      " - 3s - loss: 0.4665 - acc: 0.7715 - val_loss: 0.6833 - val_acc: 0.6091\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 0.65373\n",
      "Epoch 75/100\n",
      " - 3s - loss: 0.4792 - acc: 0.7575 - val_loss: 0.7329 - val_acc: 0.6036\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 0.65373\n",
      "Epoch 76/100\n",
      " - 3s - loss: 0.4675 - acc: 0.7714 - val_loss: 0.7181 - val_acc: 0.6136\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 0.65373\n",
      "Epoch 77/100\n",
      " - 3s - loss: 0.4367 - acc: 0.7900 - val_loss: 0.6623 - val_acc: 0.6327\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 0.65373\n",
      "Epoch 78/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 3s - loss: 0.4350 - acc: 0.7932 - val_loss: 0.8118 - val_acc: 0.5864\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 0.65373\n",
      "Epoch 79/100\n",
      " - 3s - loss: 0.4258 - acc: 0.7959 - val_loss: 0.7010 - val_acc: 0.6382\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 0.65373\n",
      "Epoch 80/100\n",
      " - 3s - loss: 0.4453 - acc: 0.7832 - val_loss: 0.7750 - val_acc: 0.6245\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 0.65373\n",
      "Epoch 81/100\n",
      " - 3s - loss: 0.4353 - acc: 0.7870 - val_loss: 0.6648 - val_acc: 0.6336\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 0.65373\n",
      "Epoch 82/100\n",
      " - 3s - loss: 0.4265 - acc: 0.7961 - val_loss: 0.6661 - val_acc: 0.6373\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 0.65373\n",
      "Epoch 83/100\n",
      " - 3s - loss: 0.4023 - acc: 0.8111 - val_loss: 0.7188 - val_acc: 0.6327\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 0.65373\n",
      "Epoch 84/100\n",
      " - 3s - loss: 0.4288 - acc: 0.7927 - val_loss: 0.7236 - val_acc: 0.6218\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 0.65373\n",
      "Epoch 85/100\n",
      " - 3s - loss: 0.3921 - acc: 0.8166 - val_loss: 0.6961 - val_acc: 0.6391\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 0.65373\n",
      "Epoch 86/100\n",
      " - 3s - loss: 0.4549 - acc: 0.7803 - val_loss: 0.6462 - val_acc: 0.6436\n",
      "\n",
      "Epoch 00086: val_loss improved from 0.65373 to 0.64616, saving model to LSTM.weights.best.hdf5\n",
      "Epoch 87/100\n",
      " - 3s - loss: 0.4028 - acc: 0.8119 - val_loss: 0.6723 - val_acc: 0.6573\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 0.64616\n",
      "Epoch 88/100\n",
      " - 3s - loss: 0.3873 - acc: 0.8227 - val_loss: 0.6874 - val_acc: 0.6391\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 0.64616\n",
      "Epoch 89/100\n",
      " - 3s - loss: 0.4097 - acc: 0.8038 - val_loss: 0.6651 - val_acc: 0.6436\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 0.64616\n",
      "Epoch 90/100\n",
      " - 3s - loss: 0.3934 - acc: 0.8162 - val_loss: 0.6880 - val_acc: 0.6164\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 0.64616\n",
      "Epoch 91/100\n",
      " - 3s - loss: 0.4103 - acc: 0.8092 - val_loss: 0.6757 - val_acc: 0.6473\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 0.64616\n",
      "Epoch 92/100\n",
      " - 3s - loss: 0.3608 - acc: 0.8374 - val_loss: 0.7075 - val_acc: 0.6509\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 0.64616\n",
      "Epoch 93/100\n",
      " - 3s - loss: 0.3694 - acc: 0.8331 - val_loss: 0.6800 - val_acc: 0.6609\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 0.64616\n",
      "Epoch 94/100\n",
      " - 3s - loss: 0.3824 - acc: 0.8227 - val_loss: 0.6741 - val_acc: 0.6555\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 0.64616\n",
      "Epoch 95/100\n",
      " - 3s - loss: 0.3608 - acc: 0.8352 - val_loss: 0.6895 - val_acc: 0.6482\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 0.64616\n",
      "Epoch 96/100\n",
      " - 3s - loss: 0.3723 - acc: 0.8294 - val_loss: 0.6750 - val_acc: 0.6600\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 0.64616\n",
      "Epoch 97/100\n",
      " - 3s - loss: 0.3435 - acc: 0.8455 - val_loss: 0.7804 - val_acc: 0.6427\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 0.64616\n",
      "Epoch 98/100\n",
      " - 3s - loss: 0.3466 - acc: 0.8441 - val_loss: 0.7484 - val_acc: 0.6691\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 0.64616\n",
      "Epoch 99/100\n",
      " - 3s - loss: 0.3525 - acc: 0.8369 - val_loss: 0.7038 - val_acc: 0.6373\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 0.64616\n",
      "Epoch 100/100\n",
      " - 3s - loss: 0.3248 - acc: 0.8517 - val_loss: 0.7084 - val_acc: 0.6664\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 0.64616\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath = 'LSTM.weights.best.hdf5', verbose = 1, save_best_only = True)\n",
    "hist = model3.fit(X_train, y_train, epochs = 100, batch_size=256, validation_split = 0.1, callbacks = [checkpointer], verbose = 2, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3253,
     "status": "ok",
     "timestamp": 1528838971145,
     "user": {
      "displayName": "SHUBHAM VERMA",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "108363614808362191748"
     },
     "user_tz": -330
    },
    "id": "BJNcF-Jx_rGD",
    "outputId": "1e48a9ea-72d5-43c8-a843-18f2c778d431"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3980/3980 [==============================] - 2s 619us/step\n",
      "Accuracy:  0.6753768843622064\n"
     ]
    }
   ],
   "source": [
    "score = model3.evaluate(X_test, y_test, verbose=1)\n",
    "print(\"Accuracy: \", score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "sUM5b7oW_qvZ"
   },
   "outputs": [],
   "source": [
    "predict4 =  [1 if a>0.5 else 0 for a in model3.predict(X_test)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "w7rvEI-nWMUY"
   },
   "source": [
    "### Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 91
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 984,
     "status": "ok",
     "timestamp": 1528839038465,
     "user": {
      "displayName": "SHUBHAM VERMA",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "108363614808362191748"
     },
     "user_tz": -330
    },
    "id": "36JspPsyZkzb",
    "outputId": "f2d84cb6-c371-4f5a-8f16-ac0363257a8e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.6753768844221105\n",
      "Precision = 0.6686774941995359\n",
      "Recall = 0.7137196631996038\n",
      "F1 Score = 0.6904647819837088\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy = {}\\nPrecision = {}\\nRecall = {}\\nF1 Score = {}\".format(metrics.accuracy_score(y_test, predict4), metrics.precision_score(y_test, predict4),metrics.recall_score(y_test, predict4),metrics.f1_score(y_test, predict4)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "-1A_bedbZo_G"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UIwPKYoN5H4j"
   },
   "source": [
    "# Improved LSTM (Tuning Parameter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 258
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2295,
     "status": "ok",
     "timestamp": 1528839050515,
     "user": {
      "displayName": "SHUBHAM VERMA",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "108363614808362191748"
     },
     "user_tz": -330
    },
    "id": "OG1XXkwD5IfU",
    "outputId": "5d447fd4-7b3d-4160-fa36-b8789e6e70ca"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "cu_dnnlstm_8 (CuDNNLSTM)     (None, 14, 512)           1054720   \n",
      "_________________________________________________________________\n",
      "cu_dnnlstm_9 (CuDNNLSTM)     (None, 512)               2101248   \n",
      "_________________________________________________________________\n",
      "dense_46 (Dense)             (None, 1)                 513       \n",
      "=================================================================\n",
      "Total params: 3,156,481\n",
      "Trainable params: 3,156,481\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model4 = Sequential()\n",
    "model4.add(CuDNNLSTM(units=512, input_shape=(14, 1), return_sequences=True))\n",
    "model4.add(CuDNNLSTM(units=512))\n",
    "model4.add(Dense(1, activation='sigmoid'))\n",
    "model4.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 258
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 959,
     "status": "ok",
     "timestamp": 1528839054269,
     "user": {
      "displayName": "SHUBHAM VERMA",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "108363614808362191748"
     },
     "user_tz": -330
    },
    "id": "SpKkcLWf7Gnr",
    "outputId": "fbb573ca-0fd7-4a07-ebf7-27a5e1dc9e72"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "cu_dnnlstm_8 (CuDNNLSTM)     (None, 14, 512)           1054720   \n",
      "_________________________________________________________________\n",
      "cu_dnnlstm_9 (CuDNNLSTM)     (None, 512)               2101248   \n",
      "_________________________________________________________________\n",
      "dense_46 (Dense)             (None, 1)                 513       \n",
      "=================================================================\n",
      "Total params: 3,156,481\n",
      "Trainable params: 3,156,481\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model4.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model4.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 22336
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 525189,
     "status": "ok",
     "timestamp": 1528839584099,
     "user": {
      "displayName": "SHUBHAM VERMA",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "108363614808362191748"
     },
     "user_tz": -330
    },
    "id": "qkVbrRz-7WaH",
    "outputId": "4ec3b388-49c2-4dda-cf4c-56070ed174db"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9900 samples, validate on 1100 samples\n",
      "Epoch 1/300\n",
      " - 3s - loss: 0.6922 - acc: 0.5243 - val_loss: 0.6918 - val_acc: 0.5255\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.69181, saving model to LSTM_new.weights.best.hdf5\n",
      "Epoch 2/300\n",
      " - 2s - loss: 0.6918 - acc: 0.5245 - val_loss: 0.6917 - val_acc: 0.5255\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.69181 to 0.69171, saving model to LSTM_new.weights.best.hdf5\n",
      "Epoch 3/300\n",
      " - 2s - loss: 0.6914 - acc: 0.5209 - val_loss: 0.6906 - val_acc: 0.5255\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.69171 to 0.69059, saving model to LSTM_new.weights.best.hdf5\n",
      "Epoch 4/300\n",
      " - 2s - loss: 0.6888 - acc: 0.5301 - val_loss: 0.6900 - val_acc: 0.5027\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.69059 to 0.69002, saving model to LSTM_new.weights.best.hdf5\n",
      "Epoch 5/300\n",
      " - 2s - loss: 0.6882 - acc: 0.5481 - val_loss: 0.6892 - val_acc: 0.5418\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.69002 to 0.68917, saving model to LSTM_new.weights.best.hdf5\n",
      "Epoch 6/300\n",
      " - 2s - loss: 0.6879 - acc: 0.5362 - val_loss: 0.6900 - val_acc: 0.5545\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.68917\n",
      "Epoch 7/300\n",
      " - 2s - loss: 0.6822 - acc: 0.5590 - val_loss: 0.6838 - val_acc: 0.5518\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.68917 to 0.68376, saving model to LSTM_new.weights.best.hdf5\n",
      "Epoch 8/300\n",
      " - 2s - loss: 0.6853 - acc: 0.5461 - val_loss: 0.6891 - val_acc: 0.5527\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.68376\n",
      "Epoch 9/300\n",
      " - 2s - loss: 0.6824 - acc: 0.5485 - val_loss: 0.8573 - val_acc: 0.4736\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.68376\n",
      "Epoch 10/300\n",
      " - 2s - loss: 0.6963 - acc: 0.5465 - val_loss: 0.6904 - val_acc: 0.5300\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.68376\n",
      "Epoch 11/300\n",
      " - 2s - loss: 0.6891 - acc: 0.5301 - val_loss: 0.6901 - val_acc: 0.5345\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.68376\n",
      "Epoch 12/300\n",
      " - 2s - loss: 0.6887 - acc: 0.5424 - val_loss: 0.6896 - val_acc: 0.5291\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.68376\n",
      "Epoch 13/300\n",
      " - 2s - loss: 0.6861 - acc: 0.5357 - val_loss: 0.6886 - val_acc: 0.5391\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.68376\n",
      "Epoch 14/300\n",
      " - 2s - loss: 0.6795 - acc: 0.5518 - val_loss: 0.6999 - val_acc: 0.5418\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.68376\n",
      "Epoch 15/300\n",
      " - 2s - loss: 0.6749 - acc: 0.5538 - val_loss: 1.5506 - val_acc: 0.5255\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.68376\n",
      "Epoch 16/300\n",
      " - 2s - loss: 0.7533 - acc: 0.5091 - val_loss: 0.6917 - val_acc: 0.5255\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.68376\n",
      "Epoch 17/300\n",
      " - 2s - loss: 0.6915 - acc: 0.5184 - val_loss: 0.6904 - val_acc: 0.5273\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.68376\n",
      "Epoch 18/300\n",
      " - 2s - loss: 0.6898 - acc: 0.5454 - val_loss: 0.6904 - val_acc: 0.5300\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.68376\n",
      "Epoch 19/300\n",
      " - 2s - loss: 0.6887 - acc: 0.5434 - val_loss: 0.6907 - val_acc: 0.5391\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.68376\n",
      "Epoch 20/300\n",
      " - 2s - loss: 0.6887 - acc: 0.5435 - val_loss: 0.6894 - val_acc: 0.5291\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.68376\n",
      "Epoch 21/300\n",
      " - 2s - loss: 0.6867 - acc: 0.5365 - val_loss: 0.6886 - val_acc: 0.5273\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.68376\n",
      "Epoch 22/300\n",
      " - 2s - loss: 0.6869 - acc: 0.5373 - val_loss: 0.6880 - val_acc: 0.5300\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.68376\n",
      "Epoch 23/300\n",
      " - 2s - loss: 0.6840 - acc: 0.5508 - val_loss: 0.6915 - val_acc: 0.5091\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.68376\n",
      "Epoch 24/300\n",
      " - 2s - loss: 0.6805 - acc: 0.5427 - val_loss: 0.6878 - val_acc: 0.5445\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.68376\n",
      "Epoch 25/300\n",
      " - 2s - loss: 0.6839 - acc: 0.5347 - val_loss: 0.7217 - val_acc: 0.4791\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.68376\n",
      "Epoch 26/300\n",
      " - 2s - loss: 0.6792 - acc: 0.5495 - val_loss: 0.6840 - val_acc: 0.5291\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.68376\n",
      "Epoch 27/300\n",
      " - 2s - loss: 0.6699 - acc: 0.5778 - val_loss: 0.6805 - val_acc: 0.5364\n",
      "\n",
      "Epoch 00027: val_loss improved from 0.68376 to 0.68047, saving model to LSTM_new.weights.best.hdf5\n",
      "Epoch 28/300\n",
      " - 2s - loss: 0.6677 - acc: 0.5733 - val_loss: 0.6943 - val_acc: 0.5464\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.68047\n",
      "Epoch 29/300\n",
      " - 2s - loss: 0.6664 - acc: 0.5776 - val_loss: 0.6778 - val_acc: 0.5664\n",
      "\n",
      "Epoch 00029: val_loss improved from 0.68047 to 0.67781, saving model to LSTM_new.weights.best.hdf5\n",
      "Epoch 30/300\n",
      " - 2s - loss: 0.6586 - acc: 0.5788 - val_loss: 0.6762 - val_acc: 0.5836\n",
      "\n",
      "Epoch 00030: val_loss improved from 0.67781 to 0.67617, saving model to LSTM_new.weights.best.hdf5\n",
      "Epoch 31/300\n",
      " - 2s - loss: 0.6837 - acc: 0.5419 - val_loss: 0.6878 - val_acc: 0.5018\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.67617\n",
      "Epoch 32/300\n",
      " - 2s - loss: 0.6808 - acc: 0.5511 - val_loss: 0.6921 - val_acc: 0.5100\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.67617\n",
      "Epoch 33/300\n",
      " - 2s - loss: 0.6724 - acc: 0.5664 - val_loss: 0.6811 - val_acc: 0.5564\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.67617\n",
      "Epoch 34/300\n",
      " - 2s - loss: 0.6647 - acc: 0.5772 - val_loss: 0.6856 - val_acc: 0.5291\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.67617\n",
      "Epoch 35/300\n",
      " - 2s - loss: 0.6671 - acc: 0.5698 - val_loss: 0.6795 - val_acc: 0.5473\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.67617\n",
      "Epoch 36/300\n",
      " - 2s - loss: 0.6537 - acc: 0.5908 - val_loss: 0.6904 - val_acc: 0.5055\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.67617\n",
      "Epoch 37/300\n",
      " - 2s - loss: 0.6501 - acc: 0.5951 - val_loss: 0.6854 - val_acc: 0.5100\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.67617\n",
      "Epoch 38/300\n",
      " - 2s - loss: 0.6498 - acc: 0.5917 - val_loss: 0.7084 - val_acc: 0.4991\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.67617\n",
      "Epoch 39/300\n",
      " - 2s - loss: 0.6603 - acc: 0.5769 - val_loss: 0.6748 - val_acc: 0.5645\n",
      "\n",
      "Epoch 00039: val_loss improved from 0.67617 to 0.67476, saving model to LSTM_new.weights.best.hdf5\n",
      "Epoch 40/300\n",
      " - 2s - loss: 0.6380 - acc: 0.6018 - val_loss: 0.6780 - val_acc: 0.5664\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.67476\n",
      "Epoch 41/300\n",
      " - 2s - loss: 0.6364 - acc: 0.5949 - val_loss: 0.6709 - val_acc: 0.5164\n",
      "\n",
      "Epoch 00041: val_loss improved from 0.67476 to 0.67090, saving model to LSTM_new.weights.best.hdf5\n",
      "Epoch 42/300\n",
      " - 2s - loss: 0.6238 - acc: 0.6057 - val_loss: 0.6612 - val_acc: 0.5627\n",
      "\n",
      "Epoch 00042: val_loss improved from 0.67090 to 0.66119, saving model to LSTM_new.weights.best.hdf5\n",
      "Epoch 43/300\n",
      " - 2s - loss: 0.6128 - acc: 0.6104 - val_loss: 0.6688 - val_acc: 0.5345\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.66119\n",
      "Epoch 44/300\n",
      " - 2s - loss: 0.6127 - acc: 0.6219 - val_loss: 0.6994 - val_acc: 0.5345\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.66119\n",
      "Epoch 45/300\n",
      " - 2s - loss: 0.6506 - acc: 0.5890 - val_loss: 0.6828 - val_acc: 0.5255\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.66119\n",
      "Epoch 46/300\n",
      " - 2s - loss: 0.6517 - acc: 0.5900 - val_loss: 0.7025 - val_acc: 0.5082\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.66119\n",
      "Epoch 47/300\n",
      " - 2s - loss: 0.6584 - acc: 0.5899 - val_loss: 0.6869 - val_acc: 0.5491\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.66119\n",
      "Epoch 48/300\n",
      " - 2s - loss: 0.6475 - acc: 0.6023 - val_loss: 0.6795 - val_acc: 0.5400\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.66119\n",
      "Epoch 49/300\n",
      " - 2s - loss: 0.6361 - acc: 0.6177 - val_loss: 0.7374 - val_acc: 0.5600\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.66119\n",
      "Epoch 50/300\n",
      " - 2s - loss: 0.6559 - acc: 0.5941 - val_loss: 0.6785 - val_acc: 0.5718\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.66119\n",
      "Epoch 51/300\n",
      " - 2s - loss: 0.6430 - acc: 0.6069 - val_loss: 0.6921 - val_acc: 0.5636\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.66119\n",
      "Epoch 52/300\n",
      " - 2s - loss: 0.7138 - acc: 0.5617 - val_loss: 0.6921 - val_acc: 0.5273\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.66119\n",
      "Epoch 53/300\n",
      " - 2s - loss: 0.6920 - acc: 0.5203 - val_loss: 0.6910 - val_acc: 0.5336\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.66119\n",
      "Epoch 54/300\n",
      " - 2s - loss: 0.6883 - acc: 0.5445 - val_loss: 0.6902 - val_acc: 0.5291\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.66119\n",
      "Epoch 55/300\n",
      " - 2s - loss: 0.6868 - acc: 0.5482 - val_loss: 0.6887 - val_acc: 0.5291\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.66119\n",
      "Epoch 56/300\n",
      " - 2s - loss: 0.6858 - acc: 0.5451 - val_loss: 0.6879 - val_acc: 0.5282\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.66119\n",
      "Epoch 57/300\n",
      " - 2s - loss: 0.6849 - acc: 0.5567 - val_loss: 0.6873 - val_acc: 0.5664\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.66119\n",
      "Epoch 58/300\n",
      " - 2s - loss: 0.6824 - acc: 0.5651 - val_loss: 0.6857 - val_acc: 0.5600\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.66119\n",
      "Epoch 59/300\n",
      " - 2s - loss: 0.6819 - acc: 0.5526 - val_loss: 0.7150 - val_acc: 0.4736\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.66119\n",
      "Epoch 60/300\n",
      " - 2s - loss: 0.6936 - acc: 0.5220 - val_loss: 0.6884 - val_acc: 0.5564\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.66119\n",
      "Epoch 61/300\n",
      " - 2s - loss: 0.6853 - acc: 0.5428 - val_loss: 0.6888 - val_acc: 0.5409\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.66119\n",
      "Epoch 62/300\n",
      " - 2s - loss: 0.6846 - acc: 0.5597 - val_loss: 0.6874 - val_acc: 0.5600\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.66119\n",
      "Epoch 63/300\n",
      " - 2s - loss: 0.6836 - acc: 0.5569 - val_loss: 0.6890 - val_acc: 0.5309\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.66119\n",
      "Epoch 64/300\n",
      " - 2s - loss: 0.6822 - acc: 0.5671 - val_loss: 0.6861 - val_acc: 0.5527\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 0.66119\n",
      "Epoch 65/300\n",
      " - 2s - loss: 0.6815 - acc: 0.5601 - val_loss: 0.6853 - val_acc: 0.5509\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 0.66119\n",
      "Epoch 66/300\n",
      " - 2s - loss: 0.6805 - acc: 0.5675 - val_loss: 0.6849 - val_acc: 0.5455\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.66119\n",
      "Epoch 67/300\n",
      " - 2s - loss: 0.6786 - acc: 0.5657 - val_loss: 0.6865 - val_acc: 0.5318\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.66119\n",
      "Epoch 68/300\n",
      " - 2s - loss: 0.6770 - acc: 0.5677 - val_loss: 0.6824 - val_acc: 0.5618\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.66119\n",
      "Epoch 69/300\n",
      " - 2s - loss: 0.6725 - acc: 0.5748 - val_loss: 0.6822 - val_acc: 0.5282\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.66119\n",
      "Epoch 70/300\n",
      " - 2s - loss: 0.6636 - acc: 0.5766 - val_loss: 0.7879 - val_acc: 0.4864\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.66119\n",
      "Epoch 71/300\n",
      " - 2s - loss: 0.6751 - acc: 0.5687 - val_loss: 0.6779 - val_acc: 0.5627\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.66119\n",
      "Epoch 72/300\n",
      " - 2s - loss: 0.6635 - acc: 0.5762 - val_loss: 0.6771 - val_acc: 0.5609\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.66119\n",
      "Epoch 73/300\n",
      " - 2s - loss: 0.6689 - acc: 0.5795 - val_loss: 0.6752 - val_acc: 0.5600\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 0.66119\n",
      "Epoch 74/300\n",
      " - 2s - loss: 0.6640 - acc: 0.5657 - val_loss: 0.6734 - val_acc: 0.5745\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 0.66119\n",
      "Epoch 75/300\n",
      " - 2s - loss: 0.6540 - acc: 0.6036 - val_loss: 0.6795 - val_acc: 0.5209\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 0.66119\n",
      "Epoch 76/300\n",
      " - 2s - loss: 0.6483 - acc: 0.5988 - val_loss: 0.6740 - val_acc: 0.5591\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 0.66119\n",
      "Epoch 77/300\n",
      " - 2s - loss: 0.6494 - acc: 0.6019 - val_loss: 0.6860 - val_acc: 0.5500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00077: val_loss did not improve from 0.66119\n",
      "Epoch 78/300\n",
      " - 2s - loss: 0.6419 - acc: 0.6049 - val_loss: 0.6731 - val_acc: 0.5691\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 0.66119\n",
      "Epoch 79/300\n",
      " - 2s - loss: 0.6323 - acc: 0.6231 - val_loss: 0.7359 - val_acc: 0.5227\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 0.66119\n",
      "Epoch 80/300\n",
      " - 2s - loss: 0.6425 - acc: 0.6081 - val_loss: 0.6857 - val_acc: 0.5191\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 0.66119\n",
      "Epoch 81/300\n",
      " - 2s - loss: 0.6495 - acc: 0.6072 - val_loss: 0.6844 - val_acc: 0.5555\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 0.66119\n",
      "Epoch 82/300\n",
      " - 2s - loss: 0.6998 - acc: 0.5766 - val_loss: 1.2323 - val_acc: 0.4745\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 0.66119\n",
      "Epoch 83/300\n",
      " - 2s - loss: 0.7361 - acc: 0.5171 - val_loss: 0.6917 - val_acc: 0.5164\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 0.66119\n",
      "Epoch 84/300\n",
      " - 2s - loss: 0.6880 - acc: 0.5371 - val_loss: 0.6943 - val_acc: 0.4918\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 0.66119\n",
      "Epoch 85/300\n",
      " - 2s - loss: 0.6905 - acc: 0.5215 - val_loss: 0.6888 - val_acc: 0.5255\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 0.66119\n",
      "Epoch 86/300\n",
      " - 2s - loss: 0.6851 - acc: 0.5478 - val_loss: 0.6881 - val_acc: 0.5455\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 0.66119\n",
      "Epoch 87/300\n",
      " - 2s - loss: 0.6803 - acc: 0.5527 - val_loss: 0.6830 - val_acc: 0.5464\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 0.66119\n",
      "Epoch 88/300\n",
      " - 2s - loss: 0.6960 - acc: 0.5392 - val_loss: 0.6906 - val_acc: 0.5282\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 0.66119\n",
      "Epoch 89/300\n",
      " - 2s - loss: 0.6841 - acc: 0.5552 - val_loss: 0.6884 - val_acc: 0.5182\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 0.66119\n",
      "Epoch 90/300\n",
      " - 2s - loss: 0.6806 - acc: 0.5629 - val_loss: 0.6846 - val_acc: 0.5318\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 0.66119\n",
      "Epoch 91/300\n",
      " - 2s - loss: 0.6758 - acc: 0.5549 - val_loss: 0.6813 - val_acc: 0.5036\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 0.66119\n",
      "Epoch 92/300\n",
      " - 2s - loss: 0.6761 - acc: 0.5578 - val_loss: 0.6875 - val_acc: 0.5436\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 0.66119\n",
      "Epoch 93/300\n",
      " - 2s - loss: 0.6895 - acc: 0.5348 - val_loss: 0.6868 - val_acc: 0.5355\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 0.66119\n",
      "Epoch 94/300\n",
      " - 2s - loss: 0.6815 - acc: 0.5437 - val_loss: 0.6831 - val_acc: 0.5264\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 0.66119\n",
      "Epoch 95/300\n",
      " - 2s - loss: 0.6728 - acc: 0.5568 - val_loss: 0.6792 - val_acc: 0.5309\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 0.66119\n",
      "Epoch 96/300\n",
      " - 2s - loss: 0.6672 - acc: 0.5593 - val_loss: 0.6767 - val_acc: 0.5345\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 0.66119\n",
      "Epoch 97/300\n",
      " - 2s - loss: 0.6695 - acc: 0.5568 - val_loss: 0.7029 - val_acc: 0.5382\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 0.66119\n",
      "Epoch 98/300\n",
      " - 2s - loss: 0.6680 - acc: 0.5690 - val_loss: 0.6975 - val_acc: 0.5027\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 0.66119\n",
      "Epoch 99/300\n",
      " - 2s - loss: 0.6652 - acc: 0.5632 - val_loss: 0.6803 - val_acc: 0.5509\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 0.66119\n",
      "Epoch 100/300\n",
      " - 2s - loss: 0.6610 - acc: 0.5772 - val_loss: 0.6849 - val_acc: 0.5327\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 0.66119\n",
      "Epoch 101/300\n",
      " - 2s - loss: 0.6604 - acc: 0.5793 - val_loss: 0.6759 - val_acc: 0.5636\n",
      "\n",
      "Epoch 00101: val_loss did not improve from 0.66119\n",
      "Epoch 102/300\n",
      " - 2s - loss: 0.6555 - acc: 0.5917 - val_loss: 0.7241 - val_acc: 0.4791\n",
      "\n",
      "Epoch 00102: val_loss did not improve from 0.66119\n",
      "Epoch 103/300\n",
      " - 2s - loss: 0.6909 - acc: 0.5324 - val_loss: 0.6866 - val_acc: 0.5291\n",
      "\n",
      "Epoch 00103: val_loss did not improve from 0.66119\n",
      "Epoch 104/300\n",
      " - 2s - loss: 0.6651 - acc: 0.5614 - val_loss: 0.6738 - val_acc: 0.5582\n",
      "\n",
      "Epoch 00104: val_loss did not improve from 0.66119\n",
      "Epoch 105/300\n",
      " - 2s - loss: 0.6534 - acc: 0.5725 - val_loss: 0.6761 - val_acc: 0.5600\n",
      "\n",
      "Epoch 00105: val_loss did not improve from 0.66119\n",
      "Epoch 106/300\n",
      " - 2s - loss: 0.6475 - acc: 0.5848 - val_loss: 0.6880 - val_acc: 0.5418\n",
      "\n",
      "Epoch 00106: val_loss did not improve from 0.66119\n",
      "Epoch 107/300\n",
      " - 2s - loss: 0.6355 - acc: 0.5973 - val_loss: 0.6653 - val_acc: 0.5573\n",
      "\n",
      "Epoch 00107: val_loss did not improve from 0.66119\n",
      "Epoch 108/300\n",
      " - 2s - loss: 0.6184 - acc: 0.5972 - val_loss: 0.6679 - val_acc: 0.5436\n",
      "\n",
      "Epoch 00108: val_loss did not improve from 0.66119\n",
      "Epoch 109/300\n",
      " - 2s - loss: 0.6164 - acc: 0.6188 - val_loss: 0.7210 - val_acc: 0.4745\n",
      "\n",
      "Epoch 00109: val_loss did not improve from 0.66119\n",
      "Epoch 110/300\n",
      " - 2s - loss: 0.6920 - acc: 0.5454 - val_loss: 0.6931 - val_acc: 0.5373\n",
      "\n",
      "Epoch 00110: val_loss did not improve from 0.66119\n",
      "Epoch 111/300\n",
      " - 2s - loss: 0.6832 - acc: 0.5289 - val_loss: 0.6931 - val_acc: 0.5473\n",
      "\n",
      "Epoch 00111: val_loss did not improve from 0.66119\n",
      "Epoch 112/300\n",
      " - 2s - loss: 0.6683 - acc: 0.5595 - val_loss: 0.7181 - val_acc: 0.5464\n",
      "\n",
      "Epoch 00112: val_loss did not improve from 0.66119\n",
      "Epoch 113/300\n",
      " - 2s - loss: 0.6771 - acc: 0.5449 - val_loss: 0.7160 - val_acc: 0.5309\n",
      "\n",
      "Epoch 00113: val_loss did not improve from 0.66119\n",
      "Epoch 114/300\n",
      " - 2s - loss: 0.6502 - acc: 0.5800 - val_loss: 0.6918 - val_acc: 0.5136\n",
      "\n",
      "Epoch 00114: val_loss did not improve from 0.66119\n",
      "Epoch 115/300\n",
      " - 2s - loss: 0.7068 - acc: 0.5425 - val_loss: 0.7002 - val_acc: 0.5255\n",
      "\n",
      "Epoch 00115: val_loss did not improve from 0.66119\n",
      "Epoch 116/300\n",
      " - 2s - loss: 0.6905 - acc: 0.5203 - val_loss: 0.6923 - val_acc: 0.5282\n",
      "\n",
      "Epoch 00116: val_loss did not improve from 0.66119\n",
      "Epoch 117/300\n",
      " - 2s - loss: 0.6856 - acc: 0.5416 - val_loss: 0.6981 - val_acc: 0.5255\n",
      "\n",
      "Epoch 00117: val_loss did not improve from 0.66119\n",
      "Epoch 118/300\n",
      " - 2s - loss: 0.6877 - acc: 0.5430 - val_loss: 0.6889 - val_acc: 0.5345\n",
      "\n",
      "Epoch 00118: val_loss did not improve from 0.66119\n",
      "Epoch 119/300\n",
      " - 2s - loss: 0.6810 - acc: 0.5598 - val_loss: 0.6919 - val_acc: 0.5364\n",
      "\n",
      "Epoch 00119: val_loss did not improve from 0.66119\n",
      "Epoch 120/300\n",
      " - 2s - loss: 0.6799 - acc: 0.5614 - val_loss: 0.6982 - val_acc: 0.5155\n",
      "\n",
      "Epoch 00120: val_loss did not improve from 0.66119\n",
      "Epoch 121/300\n",
      " - 2s - loss: 0.6819 - acc: 0.5466 - val_loss: 0.6987 - val_acc: 0.5182\n",
      "\n",
      "Epoch 00121: val_loss did not improve from 0.66119\n",
      "Epoch 122/300\n",
      " - 2s - loss: 0.6757 - acc: 0.5647 - val_loss: 0.6872 - val_acc: 0.4609\n",
      "\n",
      "Epoch 00122: val_loss did not improve from 0.66119\n",
      "Epoch 123/300\n",
      " - 2s - loss: 0.6655 - acc: 0.5535 - val_loss: 0.7267 - val_acc: 0.5327\n",
      "\n",
      "Epoch 00123: val_loss did not improve from 0.66119\n",
      "Epoch 124/300\n",
      " - 2s - loss: 0.6733 - acc: 0.5596 - val_loss: 0.6860 - val_acc: 0.5245\n",
      "\n",
      "Epoch 00124: val_loss did not improve from 0.66119\n",
      "Epoch 125/300\n",
      " - 2s - loss: 0.6671 - acc: 0.5548 - val_loss: 0.6837 - val_acc: 0.5009\n",
      "\n",
      "Epoch 00125: val_loss did not improve from 0.66119\n",
      "Epoch 126/300\n",
      " - 2s - loss: 0.6611 - acc: 0.5804 - val_loss: 0.6936 - val_acc: 0.5109\n",
      "\n",
      "Epoch 00126: val_loss did not improve from 0.66119\n",
      "Epoch 127/300\n",
      " - 2s - loss: 0.6557 - acc: 0.5934 - val_loss: 0.6838 - val_acc: 0.4955\n",
      "\n",
      "Epoch 00127: val_loss did not improve from 0.66119\n",
      "Epoch 128/300\n",
      " - 2s - loss: 0.6570 - acc: 0.5810 - val_loss: 0.6822 - val_acc: 0.5036\n",
      "\n",
      "Epoch 00128: val_loss did not improve from 0.66119\n",
      "Epoch 129/300\n",
      " - 2s - loss: 0.6488 - acc: 0.5999 - val_loss: 0.6820 - val_acc: 0.5182\n",
      "\n",
      "Epoch 00129: val_loss did not improve from 0.66119\n",
      "Epoch 130/300\n",
      " - 2s - loss: 0.6440 - acc: 0.6028 - val_loss: 0.6804 - val_acc: 0.5291\n",
      "\n",
      "Epoch 00130: val_loss did not improve from 0.66119\n",
      "Epoch 131/300\n",
      " - 2s - loss: 0.6473 - acc: 0.6097 - val_loss: 0.7152 - val_acc: 0.5209\n",
      "\n",
      "Epoch 00131: val_loss did not improve from 0.66119\n",
      "Epoch 132/300\n",
      " - 2s - loss: 0.6514 - acc: 0.5828 - val_loss: 0.6930 - val_acc: 0.5164\n",
      "\n",
      "Epoch 00132: val_loss did not improve from 0.66119\n",
      "Epoch 133/300\n",
      " - 2s - loss: 0.6411 - acc: 0.6048 - val_loss: 0.6844 - val_acc: 0.5273\n",
      "\n",
      "Epoch 00133: val_loss did not improve from 0.66119\n",
      "Epoch 134/300\n",
      " - 2s - loss: 0.6366 - acc: 0.6177 - val_loss: 0.6924 - val_acc: 0.5509\n",
      "\n",
      "Epoch 00134: val_loss did not improve from 0.66119\n",
      "Epoch 135/300\n",
      " - 2s - loss: 0.6316 - acc: 0.6133 - val_loss: 0.7648 - val_acc: 0.5227\n",
      "\n",
      "Epoch 00135: val_loss did not improve from 0.66119\n",
      "Epoch 136/300\n",
      " - 2s - loss: 0.6370 - acc: 0.6183 - val_loss: 0.7075 - val_acc: 0.5355\n",
      "\n",
      "Epoch 00136: val_loss did not improve from 0.66119\n",
      "Epoch 137/300\n",
      " - 2s - loss: 0.6325 - acc: 0.6225 - val_loss: 0.6829 - val_acc: 0.5164\n",
      "\n",
      "Epoch 00137: val_loss did not improve from 0.66119\n",
      "Epoch 138/300\n",
      " - 2s - loss: 0.6154 - acc: 0.6481 - val_loss: 0.6895 - val_acc: 0.5355\n",
      "\n",
      "Epoch 00138: val_loss did not improve from 0.66119\n",
      "Epoch 139/300\n",
      " - 2s - loss: 0.6420 - acc: 0.6229 - val_loss: 0.6759 - val_acc: 0.5191\n",
      "\n",
      "Epoch 00139: val_loss did not improve from 0.66119\n",
      "Epoch 140/300\n",
      " - 2s - loss: 0.6431 - acc: 0.5961 - val_loss: 0.6860 - val_acc: 0.5382\n",
      "\n",
      "Epoch 00140: val_loss did not improve from 0.66119\n",
      "Epoch 141/300\n",
      " - 2s - loss: 0.6298 - acc: 0.6305 - val_loss: 0.6864 - val_acc: 0.5200\n",
      "\n",
      "Epoch 00141: val_loss did not improve from 0.66119\n",
      "Epoch 142/300\n",
      " - 2s - loss: 0.6080 - acc: 0.6523 - val_loss: 0.6912 - val_acc: 0.5573\n",
      "\n",
      "Epoch 00142: val_loss did not improve from 0.66119\n",
      "Epoch 143/300\n",
      " - 2s - loss: 0.6069 - acc: 0.6378 - val_loss: 0.7193 - val_acc: 0.5355\n",
      "\n",
      "Epoch 00143: val_loss did not improve from 0.66119\n",
      "Epoch 144/300\n",
      " - 2s - loss: 0.6026 - acc: 0.6474 - val_loss: 0.6816 - val_acc: 0.5482\n",
      "\n",
      "Epoch 00144: val_loss did not improve from 0.66119\n",
      "Epoch 145/300\n",
      " - 2s - loss: 0.5958 - acc: 0.6554 - val_loss: 0.6693 - val_acc: 0.5518\n",
      "\n",
      "Epoch 00145: val_loss did not improve from 0.66119\n",
      "Epoch 146/300\n",
      " - 2s - loss: 0.6138 - acc: 0.6380 - val_loss: 0.7301 - val_acc: 0.5327\n",
      "\n",
      "Epoch 00146: val_loss did not improve from 0.66119\n",
      "Epoch 147/300\n",
      " - 2s - loss: 0.5968 - acc: 0.6610 - val_loss: 0.7054 - val_acc: 0.5245\n",
      "\n",
      "Epoch 00147: val_loss did not improve from 0.66119\n",
      "Epoch 148/300\n",
      " - 2s - loss: 0.6006 - acc: 0.6527 - val_loss: 0.7224 - val_acc: 0.5600\n",
      "\n",
      "Epoch 00148: val_loss did not improve from 0.66119\n",
      "Epoch 149/300\n",
      " - 2s - loss: 0.6101 - acc: 0.6352 - val_loss: 0.6867 - val_acc: 0.5327\n",
      "\n",
      "Epoch 00149: val_loss did not improve from 0.66119\n",
      "Epoch 150/300\n",
      " - 2s - loss: 0.5836 - acc: 0.6807 - val_loss: 0.6768 - val_acc: 0.5509\n",
      "\n",
      "Epoch 00150: val_loss did not improve from 0.66119\n",
      "Epoch 151/300\n",
      " - 2s - loss: 0.5685 - acc: 0.6815 - val_loss: 0.6626 - val_acc: 0.5755\n",
      "\n",
      "Epoch 00151: val_loss did not improve from 0.66119\n",
      "Epoch 152/300\n",
      " - 2s - loss: 0.5794 - acc: 0.6797 - val_loss: 0.6824 - val_acc: 0.5773\n",
      "\n",
      "Epoch 00152: val_loss did not improve from 0.66119\n",
      "Epoch 153/300\n",
      " - 2s - loss: 0.5659 - acc: 0.6817 - val_loss: 0.7378 - val_acc: 0.5436\n",
      "\n",
      "Epoch 00153: val_loss did not improve from 0.66119\n",
      "Epoch 154/300\n",
      " - 2s - loss: 0.5470 - acc: 0.7036 - val_loss: 0.6866 - val_acc: 0.5709\n",
      "\n",
      "Epoch 00154: val_loss did not improve from 0.66119\n",
      "Epoch 155/300\n",
      " - 2s - loss: 0.5807 - acc: 0.6776 - val_loss: 0.6712 - val_acc: 0.5873\n",
      "\n",
      "Epoch 00155: val_loss did not improve from 0.66119\n",
      "Epoch 156/300\n",
      " - 2s - loss: 0.5397 - acc: 0.7128 - val_loss: 0.6516 - val_acc: 0.5809\n",
      "\n",
      "Epoch 00156: val_loss improved from 0.66119 to 0.65158, saving model to LSTM_new.weights.best.hdf5\n",
      "Epoch 157/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 2s - loss: 0.5458 - acc: 0.7078 - val_loss: 0.7380 - val_acc: 0.5409\n",
      "\n",
      "Epoch 00157: val_loss did not improve from 0.65158\n",
      "Epoch 158/300\n",
      " - 2s - loss: 0.6747 - acc: 0.5649 - val_loss: 0.6723 - val_acc: 0.5482\n",
      "\n",
      "Epoch 00158: val_loss did not improve from 0.65158\n",
      "Epoch 159/300\n",
      " - 2s - loss: 0.6485 - acc: 0.6194 - val_loss: 0.6882 - val_acc: 0.5336\n",
      "\n",
      "Epoch 00159: val_loss did not improve from 0.65158\n",
      "Epoch 160/300\n",
      " - 2s - loss: 0.6935 - acc: 0.5443 - val_loss: 0.6803 - val_acc: 0.5800\n",
      "\n",
      "Epoch 00160: val_loss did not improve from 0.65158\n",
      "Epoch 161/300\n",
      " - 2s - loss: 0.6690 - acc: 0.6026 - val_loss: 0.6806 - val_acc: 0.5455\n",
      "\n",
      "Epoch 00161: val_loss did not improve from 0.65158\n",
      "Epoch 162/300\n",
      " - 2s - loss: 0.6645 - acc: 0.5895 - val_loss: 0.6784 - val_acc: 0.5618\n",
      "\n",
      "Epoch 00162: val_loss did not improve from 0.65158\n",
      "Epoch 163/300\n",
      " - 2s - loss: 0.6528 - acc: 0.6062 - val_loss: 0.6656 - val_acc: 0.5800\n",
      "\n",
      "Epoch 00163: val_loss did not improve from 0.65158\n",
      "Epoch 164/300\n",
      " - 2s - loss: 0.6382 - acc: 0.6103 - val_loss: 0.6748 - val_acc: 0.5573\n",
      "\n",
      "Epoch 00164: val_loss did not improve from 0.65158\n",
      "Epoch 165/300\n",
      " - 2s - loss: 0.6217 - acc: 0.6255 - val_loss: 0.6579 - val_acc: 0.5736\n",
      "\n",
      "Epoch 00165: val_loss did not improve from 0.65158\n",
      "Epoch 166/300\n",
      " - 2s - loss: 0.6128 - acc: 0.6303 - val_loss: 0.6537 - val_acc: 0.5755\n",
      "\n",
      "Epoch 00166: val_loss did not improve from 0.65158\n",
      "Epoch 167/300\n",
      " - 2s - loss: 0.6017 - acc: 0.6532 - val_loss: 0.6697 - val_acc: 0.5573\n",
      "\n",
      "Epoch 00167: val_loss did not improve from 0.65158\n",
      "Epoch 168/300\n",
      " - 2s - loss: 0.6043 - acc: 0.6459 - val_loss: 0.6817 - val_acc: 0.5891\n",
      "\n",
      "Epoch 00168: val_loss did not improve from 0.65158\n",
      "Epoch 169/300\n",
      " - 2s - loss: 0.6195 - acc: 0.6379 - val_loss: 0.8145 - val_acc: 0.5400\n",
      "\n",
      "Epoch 00169: val_loss did not improve from 0.65158\n",
      "Epoch 170/300\n",
      " - 2s - loss: 0.6651 - acc: 0.5948 - val_loss: 0.6550 - val_acc: 0.5745\n",
      "\n",
      "Epoch 00170: val_loss did not improve from 0.65158\n",
      "Epoch 171/300\n",
      " - 2s - loss: 0.6238 - acc: 0.6217 - val_loss: 0.6685 - val_acc: 0.5609\n",
      "\n",
      "Epoch 00171: val_loss did not improve from 0.65158\n",
      "Epoch 172/300\n",
      " - 2s - loss: 0.6104 - acc: 0.6211 - val_loss: 0.6589 - val_acc: 0.5645\n",
      "\n",
      "Epoch 00172: val_loss did not improve from 0.65158\n",
      "Epoch 173/300\n",
      " - 2s - loss: 0.6029 - acc: 0.6279 - val_loss: 0.6801 - val_acc: 0.5418\n",
      "\n",
      "Epoch 00173: val_loss did not improve from 0.65158\n",
      "Epoch 174/300\n",
      " - 2s - loss: 0.6011 - acc: 0.6351 - val_loss: 0.7076 - val_acc: 0.5564\n",
      "\n",
      "Epoch 00174: val_loss did not improve from 0.65158\n",
      "Epoch 175/300\n",
      " - 2s - loss: 0.6134 - acc: 0.6318 - val_loss: 0.7029 - val_acc: 0.5555\n",
      "\n",
      "Epoch 00175: val_loss did not improve from 0.65158\n",
      "Epoch 176/300\n",
      " - 2s - loss: 0.5978 - acc: 0.6381 - val_loss: 0.6664 - val_acc: 0.5445\n",
      "\n",
      "Epoch 00176: val_loss did not improve from 0.65158\n",
      "Epoch 177/300\n",
      " - 2s - loss: 0.6045 - acc: 0.6359 - val_loss: 0.7096 - val_acc: 0.5591\n",
      "\n",
      "Epoch 00177: val_loss did not improve from 0.65158\n",
      "Epoch 178/300\n",
      " - 2s - loss: 0.5875 - acc: 0.6567 - val_loss: 0.6803 - val_acc: 0.5591\n",
      "\n",
      "Epoch 00178: val_loss did not improve from 0.65158\n",
      "Epoch 179/300\n",
      " - 2s - loss: 0.5932 - acc: 0.6531 - val_loss: 0.6742 - val_acc: 0.5355\n",
      "\n",
      "Epoch 00179: val_loss did not improve from 0.65158\n",
      "Epoch 180/300\n",
      " - 2s - loss: 0.5883 - acc: 0.6641 - val_loss: 0.6907 - val_acc: 0.5627\n",
      "\n",
      "Epoch 00180: val_loss did not improve from 0.65158\n",
      "Epoch 181/300\n",
      " - 2s - loss: 0.5813 - acc: 0.6745 - val_loss: 0.6800 - val_acc: 0.5427\n",
      "\n",
      "Epoch 00181: val_loss did not improve from 0.65158\n",
      "Epoch 182/300\n",
      " - 2s - loss: 0.5833 - acc: 0.6663 - val_loss: 0.6686 - val_acc: 0.5373\n",
      "\n",
      "Epoch 00182: val_loss did not improve from 0.65158\n",
      "Epoch 183/300\n",
      " - 2s - loss: 0.5661 - acc: 0.6909 - val_loss: 0.6767 - val_acc: 0.5636\n",
      "\n",
      "Epoch 00183: val_loss did not improve from 0.65158\n",
      "Epoch 184/300\n",
      " - 2s - loss: 0.6019 - acc: 0.6825 - val_loss: 0.6965 - val_acc: 0.5482\n",
      "\n",
      "Epoch 00184: val_loss did not improve from 0.65158\n",
      "Epoch 185/300\n",
      " - 2s - loss: 0.6358 - acc: 0.6111 - val_loss: 0.6562 - val_acc: 0.5600\n",
      "\n",
      "Epoch 00185: val_loss did not improve from 0.65158\n",
      "Epoch 186/300\n",
      " - 2s - loss: 0.6046 - acc: 0.6449 - val_loss: 0.6547 - val_acc: 0.5655\n",
      "\n",
      "Epoch 00186: val_loss did not improve from 0.65158\n",
      "Epoch 187/300\n",
      " - 2s - loss: 0.5882 - acc: 0.6692 - val_loss: 0.6733 - val_acc: 0.5755\n",
      "\n",
      "Epoch 00187: val_loss did not improve from 0.65158\n",
      "Epoch 188/300\n",
      " - 2s - loss: 0.5663 - acc: 0.7053 - val_loss: 0.6730 - val_acc: 0.5327\n",
      "\n",
      "Epoch 00188: val_loss did not improve from 0.65158\n",
      "Epoch 189/300\n",
      " - 2s - loss: 0.6210 - acc: 0.6488 - val_loss: 0.6936 - val_acc: 0.5464\n",
      "\n",
      "Epoch 00189: val_loss did not improve from 0.65158\n",
      "Epoch 190/300\n",
      " - 2s - loss: 0.5922 - acc: 0.6603 - val_loss: 0.7029 - val_acc: 0.5755\n",
      "\n",
      "Epoch 00190: val_loss did not improve from 0.65158\n",
      "Epoch 191/300\n",
      " - 2s - loss: 0.5754 - acc: 0.6782 - val_loss: 0.6981 - val_acc: 0.5827\n",
      "\n",
      "Epoch 00191: val_loss did not improve from 0.65158\n",
      "Epoch 192/300\n",
      " - 2s - loss: 0.5450 - acc: 0.7143 - val_loss: 0.9020 - val_acc: 0.5327\n",
      "\n",
      "Epoch 00192: val_loss did not improve from 0.65158\n",
      "Epoch 193/300\n",
      " - 2s - loss: 0.6650 - acc: 0.6107 - val_loss: 0.6849 - val_acc: 0.5427\n",
      "\n",
      "Epoch 00193: val_loss did not improve from 0.65158\n",
      "Epoch 194/300\n",
      " - 2s - loss: 0.6111 - acc: 0.6605 - val_loss: 0.7331 - val_acc: 0.5300\n",
      "\n",
      "Epoch 00194: val_loss did not improve from 0.65158\n",
      "Epoch 195/300\n",
      " - 2s - loss: 0.5807 - acc: 0.6716 - val_loss: 0.7410 - val_acc: 0.5600\n",
      "\n",
      "Epoch 00195: val_loss did not improve from 0.65158\n",
      "Epoch 196/300\n",
      " - 2s - loss: 0.5494 - acc: 0.7079 - val_loss: 0.7083 - val_acc: 0.5536\n",
      "\n",
      "Epoch 00196: val_loss did not improve from 0.65158\n",
      "Epoch 197/300\n",
      " - 2s - loss: 0.5379 - acc: 0.7109 - val_loss: 0.8196 - val_acc: 0.5564\n",
      "\n",
      "Epoch 00197: val_loss did not improve from 0.65158\n",
      "Epoch 198/300\n",
      " - 2s - loss: 0.5943 - acc: 0.6586 - val_loss: 0.6527 - val_acc: 0.5809\n",
      "\n",
      "Epoch 00198: val_loss did not improve from 0.65158\n",
      "Epoch 199/300\n",
      " - 2s - loss: 0.5368 - acc: 0.7074 - val_loss: 0.7091 - val_acc: 0.5818\n",
      "\n",
      "Epoch 00199: val_loss did not improve from 0.65158\n",
      "Epoch 200/300\n",
      " - 2s - loss: 0.5586 - acc: 0.6866 - val_loss: 0.6763 - val_acc: 0.5555\n",
      "\n",
      "Epoch 00200: val_loss did not improve from 0.65158\n",
      "Epoch 201/300\n",
      " - 2s - loss: 0.5441 - acc: 0.7122 - val_loss: 0.6566 - val_acc: 0.5655\n",
      "\n",
      "Epoch 00201: val_loss did not improve from 0.65158\n",
      "Epoch 202/300\n",
      " - 2s - loss: 0.5183 - acc: 0.7307 - val_loss: 0.6743 - val_acc: 0.5727\n",
      "\n",
      "Epoch 00202: val_loss did not improve from 0.65158\n",
      "Epoch 203/300\n",
      " - 2s - loss: 0.5236 - acc: 0.7236 - val_loss: 0.7226 - val_acc: 0.5800\n",
      "\n",
      "Epoch 00203: val_loss did not improve from 0.65158\n",
      "Epoch 204/300\n",
      " - 2s - loss: 0.5458 - acc: 0.7159 - val_loss: 0.6789 - val_acc: 0.5745\n",
      "\n",
      "Epoch 00204: val_loss did not improve from 0.65158\n",
      "Epoch 205/300\n",
      " - 2s - loss: 0.5188 - acc: 0.7369 - val_loss: 0.7130 - val_acc: 0.5627\n",
      "\n",
      "Epoch 00205: val_loss did not improve from 0.65158\n",
      "Epoch 206/300\n",
      " - 2s - loss: 0.4997 - acc: 0.7470 - val_loss: 0.7805 - val_acc: 0.5736\n",
      "\n",
      "Epoch 00206: val_loss did not improve from 0.65158\n",
      "Epoch 207/300\n",
      " - 2s - loss: 0.6490 - acc: 0.6364 - val_loss: 0.6924 - val_acc: 0.5173\n",
      "\n",
      "Epoch 00207: val_loss did not improve from 0.65158\n",
      "Epoch 208/300\n",
      " - 2s - loss: 0.5745 - acc: 0.6740 - val_loss: 0.7122 - val_acc: 0.5300\n",
      "\n",
      "Epoch 00208: val_loss did not improve from 0.65158\n",
      "Epoch 209/300\n",
      " - 2s - loss: 0.5654 - acc: 0.6864 - val_loss: 0.7011 - val_acc: 0.5291\n",
      "\n",
      "Epoch 00209: val_loss did not improve from 0.65158\n",
      "Epoch 210/300\n",
      " - 2s - loss: 0.5633 - acc: 0.6838 - val_loss: 0.6880 - val_acc: 0.5364\n",
      "\n",
      "Epoch 00210: val_loss did not improve from 0.65158\n",
      "Epoch 211/300\n",
      " - 2s - loss: 0.5409 - acc: 0.7002 - val_loss: 0.7035 - val_acc: 0.5500\n",
      "\n",
      "Epoch 00211: val_loss did not improve from 0.65158\n",
      "Epoch 212/300\n",
      " - 2s - loss: 0.5544 - acc: 0.6935 - val_loss: 0.7239 - val_acc: 0.5627\n",
      "\n",
      "Epoch 00212: val_loss did not improve from 0.65158\n",
      "Epoch 213/300\n",
      " - 2s - loss: 0.5475 - acc: 0.6999 - val_loss: 0.6734 - val_acc: 0.5618\n",
      "\n",
      "Epoch 00213: val_loss did not improve from 0.65158\n",
      "Epoch 214/300\n",
      " - 2s - loss: 0.5027 - acc: 0.7262 - val_loss: 0.6619 - val_acc: 0.5782\n",
      "\n",
      "Epoch 00214: val_loss did not improve from 0.65158\n",
      "Epoch 215/300\n",
      " - 2s - loss: 0.4904 - acc: 0.7320 - val_loss: 0.7206 - val_acc: 0.5609\n",
      "\n",
      "Epoch 00215: val_loss did not improve from 0.65158\n",
      "Epoch 216/300\n",
      " - 2s - loss: 0.5269 - acc: 0.7263 - val_loss: 0.6932 - val_acc: 0.5627\n",
      "\n",
      "Epoch 00216: val_loss did not improve from 0.65158\n",
      "Epoch 217/300\n",
      " - 2s - loss: 0.4849 - acc: 0.7431 - val_loss: 0.7118 - val_acc: 0.6018\n",
      "\n",
      "Epoch 00217: val_loss did not improve from 0.65158\n",
      "Epoch 218/300\n",
      " - 2s - loss: 0.4957 - acc: 0.7416 - val_loss: 0.6873 - val_acc: 0.5827\n",
      "\n",
      "Epoch 00218: val_loss did not improve from 0.65158\n",
      "Epoch 219/300\n",
      " - 2s - loss: 0.5625 - acc: 0.7042 - val_loss: 0.7589 - val_acc: 0.5291\n",
      "\n",
      "Epoch 00219: val_loss did not improve from 0.65158\n",
      "Epoch 220/300\n",
      " - 2s - loss: 0.5338 - acc: 0.7211 - val_loss: 0.6743 - val_acc: 0.6100\n",
      "\n",
      "Epoch 00220: val_loss did not improve from 0.65158\n",
      "Epoch 221/300\n",
      " - 2s - loss: 0.4800 - acc: 0.7554 - val_loss: 0.6371 - val_acc: 0.6245\n",
      "\n",
      "Epoch 00221: val_loss improved from 0.65158 to 0.63708, saving model to LSTM_new.weights.best.hdf5\n",
      "Epoch 222/300\n",
      " - 2s - loss: 0.4531 - acc: 0.7726 - val_loss: 0.6493 - val_acc: 0.5900\n",
      "\n",
      "Epoch 00222: val_loss did not improve from 0.63708\n",
      "Epoch 223/300\n",
      " - 2s - loss: 0.4417 - acc: 0.7836 - val_loss: 0.6568 - val_acc: 0.6264\n",
      "\n",
      "Epoch 00223: val_loss did not improve from 0.63708\n",
      "Epoch 224/300\n",
      " - 2s - loss: 0.4561 - acc: 0.7743 - val_loss: 0.7171 - val_acc: 0.5927\n",
      "\n",
      "Epoch 00224: val_loss did not improve from 0.63708\n",
      "Epoch 225/300\n",
      " - 2s - loss: 0.4627 - acc: 0.7705 - val_loss: 0.6407 - val_acc: 0.6345\n",
      "\n",
      "Epoch 00225: val_loss did not improve from 0.63708\n",
      "Epoch 226/300\n",
      " - 2s - loss: 0.4168 - acc: 0.7986 - val_loss: 0.7824 - val_acc: 0.5791\n",
      "\n",
      "Epoch 00226: val_loss did not improve from 0.63708\n",
      "Epoch 227/300\n",
      " - 2s - loss: 0.5746 - acc: 0.6958 - val_loss: 0.6310 - val_acc: 0.6264\n",
      "\n",
      "Epoch 00227: val_loss improved from 0.63708 to 0.63101, saving model to LSTM_new.weights.best.hdf5\n",
      "Epoch 228/300\n",
      " - 2s - loss: 0.4824 - acc: 0.7627 - val_loss: 0.6461 - val_acc: 0.6518\n",
      "\n",
      "Epoch 00228: val_loss did not improve from 0.63101\n",
      "Epoch 229/300\n",
      " - 2s - loss: 0.4686 - acc: 0.7607 - val_loss: 0.6864 - val_acc: 0.5809\n",
      "\n",
      "Epoch 00229: val_loss did not improve from 0.63101\n",
      "Epoch 230/300\n",
      " - 2s - loss: 0.4725 - acc: 0.7574 - val_loss: 0.7450 - val_acc: 0.5973\n",
      "\n",
      "Epoch 00230: val_loss did not improve from 0.63101\n",
      "Epoch 231/300\n",
      " - 2s - loss: 0.4515 - acc: 0.7826 - val_loss: 0.6105 - val_acc: 0.6673\n",
      "\n",
      "Epoch 00231: val_loss improved from 0.63101 to 0.61048, saving model to LSTM_new.weights.best.hdf5\n",
      "Epoch 232/300\n",
      " - 2s - loss: 0.4054 - acc: 0.8106 - val_loss: 0.6219 - val_acc: 0.6509\n",
      "\n",
      "Epoch 00232: val_loss did not improve from 0.61048\n",
      "Epoch 233/300\n",
      " - 2s - loss: 0.4186 - acc: 0.7965 - val_loss: 0.6749 - val_acc: 0.6191\n",
      "\n",
      "Epoch 00233: val_loss did not improve from 0.61048\n",
      "Epoch 234/300\n",
      " - 2s - loss: 0.4046 - acc: 0.8006 - val_loss: 0.6626 - val_acc: 0.6282\n",
      "\n",
      "Epoch 00234: val_loss did not improve from 0.61048\n",
      "Epoch 235/300\n",
      " - 2s - loss: 0.4297 - acc: 0.7918 - val_loss: 0.5970 - val_acc: 0.6864\n",
      "\n",
      "Epoch 00235: val_loss improved from 0.61048 to 0.59696, saving model to LSTM_new.weights.best.hdf5\n",
      "Epoch 236/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 2s - loss: 0.3976 - acc: 0.8073 - val_loss: 0.6927 - val_acc: 0.6491\n",
      "\n",
      "Epoch 00236: val_loss did not improve from 0.59696\n",
      "Epoch 237/300\n",
      " - 2s - loss: 0.4215 - acc: 0.7894 - val_loss: 0.6716 - val_acc: 0.6273\n",
      "\n",
      "Epoch 00237: val_loss did not improve from 0.59696\n",
      "Epoch 238/300\n",
      " - 2s - loss: 0.4548 - acc: 0.7727 - val_loss: 0.7001 - val_acc: 0.6327\n",
      "\n",
      "Epoch 00238: val_loss did not improve from 0.59696\n",
      "Epoch 239/300\n",
      " - 2s - loss: 0.4452 - acc: 0.7881 - val_loss: 0.6165 - val_acc: 0.6536\n",
      "\n",
      "Epoch 00239: val_loss did not improve from 0.59696\n",
      "Epoch 240/300\n",
      " - 2s - loss: 0.3833 - acc: 0.8159 - val_loss: 0.5975 - val_acc: 0.6982\n",
      "\n",
      "Epoch 00240: val_loss did not improve from 0.59696\n",
      "Epoch 241/300\n",
      " - 2s - loss: 0.3884 - acc: 0.8205 - val_loss: 0.6234 - val_acc: 0.6436\n",
      "\n",
      "Epoch 00241: val_loss did not improve from 0.59696\n",
      "Epoch 242/300\n",
      " - 2s - loss: 0.3756 - acc: 0.8222 - val_loss: 0.5671 - val_acc: 0.7209\n",
      "\n",
      "Epoch 00242: val_loss improved from 0.59696 to 0.56707, saving model to LSTM_new.weights.best.hdf5\n",
      "Epoch 243/300\n",
      " - 2s - loss: 0.3636 - acc: 0.8322 - val_loss: 0.5964 - val_acc: 0.7073\n",
      "\n",
      "Epoch 00243: val_loss did not improve from 0.56707\n",
      "Epoch 244/300\n",
      " - 2s - loss: 0.4622 - acc: 0.7681 - val_loss: 0.6710 - val_acc: 0.6227\n",
      "\n",
      "Epoch 00244: val_loss did not improve from 0.56707\n",
      "Epoch 245/300\n",
      " - 2s - loss: 0.3979 - acc: 0.8171 - val_loss: 0.6492 - val_acc: 0.6564\n",
      "\n",
      "Epoch 00245: val_loss did not improve from 0.56707\n",
      "Epoch 246/300\n",
      " - 2s - loss: 0.3683 - acc: 0.8256 - val_loss: 0.5626 - val_acc: 0.7082\n",
      "\n",
      "Epoch 00246: val_loss improved from 0.56707 to 0.56259, saving model to LSTM_new.weights.best.hdf5\n",
      "Epoch 247/300\n",
      " - 2s - loss: 0.3660 - acc: 0.8252 - val_loss: 0.7674 - val_acc: 0.6382\n",
      "\n",
      "Epoch 00247: val_loss did not improve from 0.56259\n",
      "Epoch 248/300\n",
      " - 2s - loss: 0.5018 - acc: 0.7568 - val_loss: 0.6030 - val_acc: 0.6891\n",
      "\n",
      "Epoch 00248: val_loss did not improve from 0.56259\n",
      "Epoch 249/300\n",
      " - 2s - loss: 0.4021 - acc: 0.8107 - val_loss: 0.5801 - val_acc: 0.7027\n",
      "\n",
      "Epoch 00249: val_loss did not improve from 0.56259\n",
      "Epoch 250/300\n",
      " - 2s - loss: 0.3539 - acc: 0.8363 - val_loss: 0.5753 - val_acc: 0.7118\n",
      "\n",
      "Epoch 00250: val_loss did not improve from 0.56259\n",
      "Epoch 251/300\n",
      " - 2s - loss: 0.3959 - acc: 0.8081 - val_loss: 0.6380 - val_acc: 0.6855\n",
      "\n",
      "Epoch 00251: val_loss did not improve from 0.56259\n",
      "Epoch 252/300\n",
      " - 2s - loss: 0.3659 - acc: 0.8341 - val_loss: 0.6119 - val_acc: 0.6973\n",
      "\n",
      "Epoch 00252: val_loss did not improve from 0.56259\n",
      "Epoch 253/300\n",
      " - 2s - loss: 0.3540 - acc: 0.8287 - val_loss: 0.5219 - val_acc: 0.7436\n",
      "\n",
      "Epoch 00253: val_loss improved from 0.56259 to 0.52195, saving model to LSTM_new.weights.best.hdf5\n",
      "Epoch 254/300\n",
      " - 2s - loss: 0.3125 - acc: 0.8594 - val_loss: 0.5661 - val_acc: 0.7345\n",
      "\n",
      "Epoch 00254: val_loss did not improve from 0.52195\n",
      "Epoch 255/300\n",
      " - 2s - loss: 0.3146 - acc: 0.8523 - val_loss: 0.5300 - val_acc: 0.7400\n",
      "\n",
      "Epoch 00255: val_loss did not improve from 0.52195\n",
      "Epoch 256/300\n",
      " - 2s - loss: 0.3043 - acc: 0.8591 - val_loss: 0.6901 - val_acc: 0.6836\n",
      "\n",
      "Epoch 00256: val_loss did not improve from 0.52195\n",
      "Epoch 257/300\n",
      " - 2s - loss: 0.4153 - acc: 0.8004 - val_loss: 0.6174 - val_acc: 0.6555\n",
      "\n",
      "Epoch 00257: val_loss did not improve from 0.52195\n",
      "Epoch 258/300\n",
      " - 2s - loss: 0.3628 - acc: 0.8292 - val_loss: 0.6088 - val_acc: 0.7100\n",
      "\n",
      "Epoch 00258: val_loss did not improve from 0.52195\n",
      "Epoch 259/300\n",
      " - 2s - loss: 0.3848 - acc: 0.8016 - val_loss: 0.5524 - val_acc: 0.7145\n",
      "\n",
      "Epoch 00259: val_loss did not improve from 0.52195\n",
      "Epoch 260/300\n",
      " - 2s - loss: 0.3337 - acc: 0.8454 - val_loss: 0.5601 - val_acc: 0.7355\n",
      "\n",
      "Epoch 00260: val_loss did not improve from 0.52195\n",
      "Epoch 261/300\n",
      " - 2s - loss: 0.3210 - acc: 0.8460 - val_loss: 0.5448 - val_acc: 0.7291\n",
      "\n",
      "Epoch 00261: val_loss did not improve from 0.52195\n",
      "Epoch 262/300\n",
      " - 2s - loss: 0.3083 - acc: 0.8555 - val_loss: 0.5483 - val_acc: 0.7200\n",
      "\n",
      "Epoch 00262: val_loss did not improve from 0.52195\n",
      "Epoch 263/300\n",
      " - 2s - loss: 0.3326 - acc: 0.8422 - val_loss: 0.7639 - val_acc: 0.7173\n",
      "\n",
      "Epoch 00263: val_loss did not improve from 0.52195\n",
      "Epoch 264/300\n",
      " - 2s - loss: 0.4598 - acc: 0.7770 - val_loss: 0.7526 - val_acc: 0.6245\n",
      "\n",
      "Epoch 00264: val_loss did not improve from 0.52195\n",
      "Epoch 265/300\n",
      " - 2s - loss: 0.4235 - acc: 0.7911 - val_loss: 0.5764 - val_acc: 0.6818\n",
      "\n",
      "Epoch 00265: val_loss did not improve from 0.52195\n",
      "Epoch 266/300\n",
      " - 2s - loss: 0.3543 - acc: 0.8327 - val_loss: 0.5121 - val_acc: 0.7455\n",
      "\n",
      "Epoch 00266: val_loss improved from 0.52195 to 0.51211, saving model to LSTM_new.weights.best.hdf5\n",
      "Epoch 267/300\n",
      " - 2s - loss: 0.3410 - acc: 0.8388 - val_loss: 0.5186 - val_acc: 0.7409\n",
      "\n",
      "Epoch 00267: val_loss did not improve from 0.51211\n",
      "Epoch 268/300\n",
      " - 2s - loss: 0.3161 - acc: 0.8491 - val_loss: 0.4815 - val_acc: 0.7727\n",
      "\n",
      "Epoch 00268: val_loss improved from 0.51211 to 0.48148, saving model to LSTM_new.weights.best.hdf5\n",
      "Epoch 269/300\n",
      " - 2s - loss: 0.3030 - acc: 0.8596 - val_loss: 0.5304 - val_acc: 0.7573\n",
      "\n",
      "Epoch 00269: val_loss did not improve from 0.48148\n",
      "Epoch 270/300\n",
      " - 2s - loss: 0.3005 - acc: 0.8573 - val_loss: 0.5183 - val_acc: 0.7536\n",
      "\n",
      "Epoch 00270: val_loss did not improve from 0.48148\n",
      "Epoch 271/300\n",
      " - 2s - loss: 0.3040 - acc: 0.8565 - val_loss: 0.6014 - val_acc: 0.7091\n",
      "\n",
      "Epoch 00271: val_loss did not improve from 0.48148\n",
      "Epoch 272/300\n",
      " - 2s - loss: 0.3160 - acc: 0.8536 - val_loss: 0.5288 - val_acc: 0.7491\n",
      "\n",
      "Epoch 00272: val_loss did not improve from 0.48148\n",
      "Epoch 273/300\n",
      " - 2s - loss: 0.3082 - acc: 0.8566 - val_loss: 0.6085 - val_acc: 0.7118\n",
      "\n",
      "Epoch 00273: val_loss did not improve from 0.48148\n",
      "Epoch 274/300\n",
      " - 2s - loss: 0.3176 - acc: 0.8481 - val_loss: 0.4860 - val_acc: 0.7727\n",
      "\n",
      "Epoch 00274: val_loss did not improve from 0.48148\n",
      "Epoch 275/300\n",
      " - 2s - loss: 0.2841 - acc: 0.8684 - val_loss: 0.5341 - val_acc: 0.7364\n",
      "\n",
      "Epoch 00275: val_loss did not improve from 0.48148\n",
      "Epoch 276/300\n",
      " - 2s - loss: 0.2651 - acc: 0.8783 - val_loss: 0.5998 - val_acc: 0.7345\n",
      "\n",
      "Epoch 00276: val_loss did not improve from 0.48148\n",
      "Epoch 277/300\n",
      " - 2s - loss: 0.2719 - acc: 0.8715 - val_loss: 0.5557 - val_acc: 0.7291\n",
      "\n",
      "Epoch 00277: val_loss did not improve from 0.48148\n",
      "Epoch 278/300\n",
      " - 2s - loss: 0.2847 - acc: 0.8704 - val_loss: 0.6444 - val_acc: 0.6873\n",
      "\n",
      "Epoch 00278: val_loss did not improve from 0.48148\n",
      "Epoch 279/300\n",
      " - 2s - loss: 0.2943 - acc: 0.8628 - val_loss: 0.5414 - val_acc: 0.7482\n",
      "\n",
      "Epoch 00279: val_loss did not improve from 0.48148\n",
      "Epoch 280/300\n",
      " - 2s - loss: 0.2670 - acc: 0.8798 - val_loss: 0.5177 - val_acc: 0.7709\n",
      "\n",
      "Epoch 00280: val_loss did not improve from 0.48148\n",
      "Epoch 281/300\n",
      " - 2s - loss: 0.2507 - acc: 0.8867 - val_loss: 0.5144 - val_acc: 0.7764\n",
      "\n",
      "Epoch 00281: val_loss did not improve from 0.48148\n",
      "Epoch 282/300\n",
      " - 2s - loss: 0.2428 - acc: 0.8894 - val_loss: 0.5116 - val_acc: 0.7836\n",
      "\n",
      "Epoch 00282: val_loss did not improve from 0.48148\n",
      "Epoch 283/300\n",
      " - 2s - loss: 0.2653 - acc: 0.8794 - val_loss: 0.6022 - val_acc: 0.7264\n",
      "\n",
      "Epoch 00283: val_loss did not improve from 0.48148\n",
      "Epoch 284/300\n",
      " - 2s - loss: 0.3013 - acc: 0.8637 - val_loss: 0.5600 - val_acc: 0.7145\n",
      "\n",
      "Epoch 00284: val_loss did not improve from 0.48148\n",
      "Epoch 285/300\n",
      " - 2s - loss: 0.2549 - acc: 0.8816 - val_loss: 0.5892 - val_acc: 0.7545\n",
      "\n",
      "Epoch 00285: val_loss did not improve from 0.48148\n",
      "Epoch 286/300\n",
      " - 2s - loss: 0.2664 - acc: 0.8768 - val_loss: 0.5985 - val_acc: 0.7527\n",
      "\n",
      "Epoch 00286: val_loss did not improve from 0.48148\n",
      "Epoch 287/300\n",
      " - 2s - loss: 0.2513 - acc: 0.8833 - val_loss: 0.5495 - val_acc: 0.7609\n",
      "\n",
      "Epoch 00287: val_loss did not improve from 0.48148\n",
      "Epoch 288/300\n",
      " - 2s - loss: 0.2833 - acc: 0.8677 - val_loss: 0.5301 - val_acc: 0.7618\n",
      "\n",
      "Epoch 00288: val_loss did not improve from 0.48148\n",
      "Epoch 289/300\n",
      " - 2s - loss: 0.2638 - acc: 0.8812 - val_loss: 0.6375 - val_acc: 0.7218\n",
      "\n",
      "Epoch 00289: val_loss did not improve from 0.48148\n",
      "Epoch 290/300\n",
      " - 2s - loss: 0.3416 - acc: 0.8387 - val_loss: 0.5804 - val_acc: 0.7764\n",
      "\n",
      "Epoch 00290: val_loss did not improve from 0.48148\n",
      "Epoch 291/300\n",
      " - 2s - loss: 0.2719 - acc: 0.8810 - val_loss: 0.5739 - val_acc: 0.7673\n",
      "\n",
      "Epoch 00291: val_loss did not improve from 0.48148\n",
      "Epoch 292/300\n",
      " - 2s - loss: 0.2549 - acc: 0.8868 - val_loss: 0.5742 - val_acc: 0.7591\n",
      "\n",
      "Epoch 00292: val_loss did not improve from 0.48148\n",
      "Epoch 293/300\n",
      " - 2s - loss: 0.2550 - acc: 0.8840 - val_loss: 0.5576 - val_acc: 0.7418\n",
      "\n",
      "Epoch 00293: val_loss did not improve from 0.48148\n",
      "Epoch 294/300\n",
      " - 2s - loss: 0.2533 - acc: 0.8885 - val_loss: 0.5230 - val_acc: 0.7545\n",
      "\n",
      "Epoch 00294: val_loss did not improve from 0.48148\n",
      "Epoch 295/300\n",
      " - 2s - loss: 0.2680 - acc: 0.8763 - val_loss: 0.5948 - val_acc: 0.7573\n",
      "\n",
      "Epoch 00295: val_loss did not improve from 0.48148\n",
      "Epoch 296/300\n",
      " - 2s - loss: 0.2458 - acc: 0.8905 - val_loss: 0.5301 - val_acc: 0.7691\n",
      "\n",
      "Epoch 00296: val_loss did not improve from 0.48148\n",
      "Epoch 297/300\n",
      " - 2s - loss: 0.2446 - acc: 0.8880 - val_loss: 0.6009 - val_acc: 0.7091\n",
      "\n",
      "Epoch 00297: val_loss did not improve from 0.48148\n",
      "Epoch 298/300\n",
      " - 2s - loss: 0.2528 - acc: 0.8842 - val_loss: 0.5446 - val_acc: 0.7591\n",
      "\n",
      "Epoch 00298: val_loss did not improve from 0.48148\n",
      "Epoch 299/300\n",
      " - 2s - loss: 0.2403 - acc: 0.8913 - val_loss: 0.5794 - val_acc: 0.7373\n",
      "\n",
      "Epoch 00299: val_loss did not improve from 0.48148\n",
      "Epoch 300/300\n",
      " - 2s - loss: 0.2791 - acc: 0.8714 - val_loss: 0.5226 - val_acc: 0.7709\n",
      "\n",
      "Epoch 00300: val_loss did not improve from 0.48148\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath = 'LSTM_new.weights.best.hdf5', verbose = 1, save_best_only = True)\n",
    "hist = model4.fit(X_train, y_train, epochs = 300, batch_size= 512, validation_split = 0.1, callbacks = [checkpointer], verbose = 2, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1870,
     "status": "ok",
     "timestamp": 1528839612276,
     "user": {
      "displayName": "SHUBHAM VERMA",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "108363614808362191748"
     },
     "user_tz": -330
    },
    "id": "JQugXwes_aIR",
    "outputId": "934c2289-2e6a-4b99-da80-6ddeda6e442c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3980/3980 [==============================] - 1s 256us/step\n",
      "Accuracy:  0.728140703577492\n"
     ]
    }
   ],
   "source": [
    "score = model4.evaluate(X_test, y_test, verbose=1)\n",
    "print(\"Accuracy: \", score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "UVV2ZFc-EcB-"
   },
   "outputs": [],
   "source": [
    "predict5 =  [1 if a>0.5 else 0 for a in model4.predict(X_test)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 91
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 941,
     "status": "ok",
     "timestamp": 1528839807810,
     "user": {
      "displayName": "SHUBHAM VERMA",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "108363614808362191748"
     },
     "user_tz": -330
    },
    "id": "xdUgnqHrDeog",
    "outputId": "17343632-1328-4401-ebfb-f82a7a51fbc0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.728140703517588\n",
      "Precision = 0.6999573196756296\n",
      "Recall = 0.8122833085685983\n",
      "F1 Score = 0.7519486474094451\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy = {}\\nPrecision = {}\\nRecall = {}\\nF1 Score = {}\".format(metrics.accuracy_score(y_test, predict5), metrics.precision_score(y_test, predict5),metrics.recall_score(y_test, predict5),metrics.f1_score(y_test, predict5)))\n",
    "score_p.append([metrics.accuracy_score(y_test, predict5), metrics.precision_score(y_test, predict5),metrics.recall_score(y_test, predict5),metrics.f1_score(y_test, predict5)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 433
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1654,
     "status": "ok",
     "timestamp": 1528840923415,
     "user": {
      "displayName": "SHUBHAM VERMA",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "108363614808362191748"
     },
     "user_tz": -330
    },
    "id": "1DdDgA0tBBaA",
    "outputId": "dc119fd8-07e2-4426-cbb7-d90d6bbe89bd"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAicAAAGgCAYAAABxDccgAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f7585bca828>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(hist.history['loss'], label = 'train')\n",
    "plt.plot(hist.history['val_loss'], label = 'valid')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "abJ6a5A5WQsw"
   },
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_ynCq160WxHd"
   },
   "source": [
    "## Ploting the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 555
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1318,
     "status": "ok",
     "timestamp": 1528839815479,
     "user": {
      "displayName": "SHUBHAM VERMA",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "108363614808362191748"
     },
     "user_tz": -330
    },
    "id": "fqXg8759Wy4c",
    "outputId": "77825d5f-3b6b-4d55-8bd6-cacb7325f2fe"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAzsAAAIGCAYAAABzvJHoAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n3cn3W5AAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f7585d67710>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#For plotting the results code is copied from Udacity Finding Donors for Charity ML Project\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt; plt.rcdefaults()\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "objects = ['Accuracy score', 'Precision score', 'Recall score', 'F1 score']\n",
    "w =  0.15\n",
    "y_pos = np.arange(len(objects))\n",
    "\n",
    "color = ['#A00000','#00A0A0','#00A000', '#FFFF00']\n",
    "s = 0\n",
    "for i in range(len(score_p)):\n",
    "    plt.bar(y_pos+s, score_p[i], color = color[i], width = w )\n",
    "    s += w\n",
    "    \n",
    "plt.xticks(y_pos + 0.22, objects)\n",
    "plt.ylabel('Scores')\n",
    "plt.title('Performance')\n",
    "plt.suptitle(\"Performance Metrics for Models\", fontsize = 16, y = 1.10)\n",
    "\n",
    "p = ['SVM',  'Logistic Regression', 'Neural Network', 'LSTM']\n",
    "\n",
    "patches = []\n",
    "\n",
    "for k in range(len(p)):\n",
    "    patches.append(mpatches.Patch(color = color[k], label = p[k]))\n",
    "\n",
    "plt.legend(handles = patches, bbox_to_anchor = (0.5, 1.19),loc = 'upper center', borderaxespad = 0., ncol = 4, fontsize = 'x-large')\n",
    "\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "ZQLo40OUYRtr"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "default_view": {},
   "name": "Capstone.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
